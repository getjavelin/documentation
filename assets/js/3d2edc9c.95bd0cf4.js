"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[6574],{5680:(e,n,a)=>{a.d(n,{xA:()=>u,yg:()=>y});var t=a(6540);function r(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function o(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,t)}return a}function l(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?o(Object(a),!0).forEach((function(n){r(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function i(e,n){if(null==e)return{};var a,t,r=function(e,n){if(null==e)return{};var a,t,r={},o=Object.keys(e);for(t=0;t<o.length;t++)a=o[t],n.indexOf(a)>=0||(r[a]=e[a]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(t=0;t<o.length;t++)a=o[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=t.createContext({}),p=function(e){var n=t.useContext(s),a=n;return e&&(a="function"==typeof e?e(n):l(l({},n),e)),a},u=function(e){var n=p(e.components);return t.createElement(s.Provider,{value:n},e.children)},c="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},g=t.forwardRef((function(e,n){var a=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),c=p(a),g=r,y=c["".concat(s,".").concat(g)]||c[g]||m[g]||o;return a?t.createElement(y,l(l({ref:n},u),{},{components:a})):t.createElement(y,l({ref:n},u))}));function y(e,n){var a=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=a.length,l=new Array(o);l[0]=g;var i={};for(var s in n)hasOwnProperty.call(n,s)&&(i[s]=n[s]);i.originalType=e,i[c]="string"==typeof e?e:r,l[1]=i;for(var p=2;p<o;p++)l[p]=a[p];return t.createElement.apply(null,l)}return t.createElement.apply(null,a)}g.displayName="MDXCreateElement"},9365:(e,n,a)=>{a.d(n,{A:()=>l});var t=a(6540),r=a(53);const o={tabItem:"tabItem_Ymn6"};function l(e){let{children:n,hidden:a,className:l}=e;return t.createElement("div",{role:"tabpanel",className:(0,r.A)(o.tabItem,l),hidden:a},n)}},1470:(e,n,a)=>{a.d(n,{A:()=>A});var t=a(8168),r=a(6540),o=a(53),l=a(3104),i=a(6347),s=a(7485),p=a(1682),u=a(9466);function c(e){return function(e){return r.Children.map(e,(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:n,label:a,attributes:t,default:r}}=e;return{value:n,label:a,attributes:t,default:r}}))}function m(e){const{values:n,children:a}=e;return(0,r.useMemo)((()=>{const e=n??c(a);return function(e){const n=(0,p.X)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,a])}function g(e){let{value:n,tabValues:a}=e;return a.some((e=>e.value===n))}function y(e){let{queryString:n=!1,groupId:a}=e;const t=(0,i.W6)(),o=function(e){let{queryString:n=!1,groupId:a}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:n,groupId:a});return[(0,s.aZ)(o),(0,r.useCallback)((e=>{if(!o)return;const n=new URLSearchParams(t.location.search);n.set(o,e),t.replace({...t.location,search:n.toString()})}),[o,t])]}function d(e){const{defaultValue:n,queryString:a=!1,groupId:t}=e,o=m(e),[l,i]=(0,r.useState)((()=>function(e){let{defaultValue:n,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!g({value:n,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const t=a.find((e=>e.default))??a[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:o}))),[s,p]=y({queryString:a,groupId:t}),[c,d]=function(e){let{groupId:n}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(n),[t,o]=(0,u.Dv)(a);return[t,(0,r.useCallback)((e=>{a&&o.set(e)}),[a,o])]}({groupId:t}),h=(()=>{const e=s??c;return g({value:e,tabValues:o})?e:null})();(0,r.useLayoutEffect)((()=>{h&&i(h)}),[h]);return{selectedValue:l,selectValue:(0,r.useCallback)((e=>{if(!g({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);i(e),p(e),d(e)}),[p,d,o]),tabValues:o}}var h=a(2303);const f={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function v(e){let{className:n,block:a,selectedValue:i,selectValue:s,tabValues:p}=e;const u=[],{blockElementScrollPositionUntilNextRender:c}=(0,l.a_)(),m=e=>{const n=e.currentTarget,a=u.indexOf(n),t=p[a].value;t!==i&&(c(n),s(t))},g=e=>{let n=null;switch(e.key){case"Enter":m(e);break;case"ArrowRight":{const a=u.indexOf(e.currentTarget)+1;n=u[a]??u[0];break}case"ArrowLeft":{const a=u.indexOf(e.currentTarget)-1;n=u[a]??u[u.length-1];break}}n?.focus()};return r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.A)("tabs",{"tabs--block":a},n)},p.map((e=>{let{value:n,label:a,attributes:l}=e;return r.createElement("li",(0,t.A)({role:"tab",tabIndex:i===n?0:-1,"aria-selected":i===n,key:n,ref:e=>u.push(e),onKeyDown:g,onClick:m},l,{className:(0,o.A)("tabs__item",f.tabItem,l?.className,{"tabs__item--active":i===n})}),a??n)})))}function _(e){let{lazy:n,children:a,selectedValue:t}=e;const o=(Array.isArray(a)?a:[a]).filter(Boolean);if(n){const e=o.find((e=>e.props.value===t));return e?(0,r.cloneElement)(e,{className:"margin-top--md"}):null}return r.createElement("div",{className:"margin-top--md"},o.map(((e,n)=>(0,r.cloneElement)(e,{key:n,hidden:e.props.value!==t}))))}function b(e){const n=d(e);return r.createElement("div",{className:(0,o.A)("tabs-container",f.tabList)},r.createElement(v,(0,t.A)({},e,n)),r.createElement(_,(0,t.A)({},e,n)))}function A(e){const n=(0,h.A)();return r.createElement(b,(0,t.A)({key:String(n)},e))}},5208:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>u,contentTitle:()=>s,default:()=>y,frontMatter:()=>i,metadata:()=>p,toc:()=>c});var t=a(8168),r=(a(6540),a(5680)),o=a(1470),l=a(9365);const i={},s="Applications",p={unversionedId:"javelin-core/integration",id:"javelin-core/integration",title:"Applications",description:"Its easy to integrate applications that leverage LLMs with Javelin. We have made it easy to seamlessly connect your applications to route all LLM traffic through Javelin with 3 lines of code change.",source:"@site/docs/javelin-core/integration.md",sourceDirName:"javelin-core",slug:"/javelin-core/integration",permalink:"/docs/javelin-core/integration",draft:!1,editUrl:"https://github.com/getjavelin/documentation/tree/main/docs/javelin-core/integration.md",tags:[],version:"current",frontMatter:{},sidebar:"someSidebar",previous:{title:"Supported Large Language Models(LLMs)",permalink:"/docs/javelin-core/supported-llms"},next:{title:"Quickstart",permalink:"/docs/javelin-langchain-python/quickstart"}},u={},c=[{value:"Leveraging the Javelin Platform",id:"leveraging-the-javelin-platform",level:2},{value:"Querying an LLM",id:"querying-an-llm",level:2},{value:"REST API",id:"rest-api",level:3},{value:"Python",id:"python",level:3},{value:"JavaScript/TypeScript",id:"javascripttypescript",level:3}],m={toc:c},g="wrapper";function y(e){let{components:n,...a}=e;return(0,r.yg)(g,(0,t.A)({},m,a,{components:n,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"applications"},"Applications"),(0,r.yg)("p",null,"Its easy to integrate applications that leverage LLMs with Javelin. We have made it easy to seamlessly connect your applications to route all LLM traffic through Javelin with 3 lines of code change."),(0,r.yg)("h2",{id:"leveraging-the-javelin-platform"},"Leveraging the Javelin Platform"),(0,r.yg)("p",null,"Rather than having your LLM Applications (like Co-Pilot apps etc.,) individually & directly point to the LLM Vendor & Model (like OpenAI, Gemini etc.,), configure the provider/model endpoint to be your Javelin endpoint. This ensures that all applications that leverage AI Models will route their requests through the gateway. Javelin supports all the ",(0,r.yg)("a",{parentName:"p",href:"supported-llms"},"latest models and providers"),", so you don't have to make any changes to your application or how requests to models are sent. "),(0,r.yg)("p",null,"See ",(0,r.yg)("a",{parentName:"p",href:"../javelin-python/quickstart"},"Python SDK")," for details on how you can easily embed this within your AI Apps. "),(0,r.yg)("p",null,"See ",(0,r.yg)("a",{parentName:"p",href:"routeconfiguration"},"Javelin Configuration")," section, for details on how to setup routes on the gateway to different models and providers. "),(0,r.yg)("p",null,"As cloud computing advances, those involved with Kubernetes should pay attention to the evolving Gateway API. Check out ",(0,r.yg)("a",{parentName:"p",href:"https://gateway-api.sigs.k8s.io/"},"Gateway API")," for detailed insights into its concepts and explore various ",(0,r.yg)("a",{parentName:"p",href:"https://gateway-api.sigs.k8s.io/#use-cases"},"Use Cases"),". We are building Javelin to provide a robust framework for managing these interactions seamlessly for new AI-driven applications or integrating AI capabilities into existing solutions."),(0,r.yg)("h2",{id:"querying-an-llm"},"Querying an LLM"),(0,r.yg)("p",null,"Javelin may send a request to one or more models based on the configured policies and route configurations and return back a response."),(0,r.yg)("h3",{id:"rest-api"},"REST API"),(0,r.yg)(o.A,{mdxType:"Tabs"},(0,r.yg)(l.A,{value:"shell",label:"curl",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell"},'curl -X POST \\\n-H "Content-Type: application/json" \\\n-H "x-api-key: $JAVELIN_API_KEY" \\\n-H "Authorization: Bearer $OPENAI_API_KEY" \\\n-d \'{\n  "messages": [\n    {\n      "role": "system",\n      "content": "Hello, you are a helpful scientific assistant."\n    },\n    {\n      "role": "user",\n      "content": "What is the chemical composition of sugar?"\n    }\n  ],\n  "temperature": 0.8\n}\' \\\n"https://api.javelin.live/v1/query/{routename}"\n')))),(0,r.yg)("h3",{id:"python"},"Python"),(0,r.yg)(o.A,{mdxType:"Tabs"},(0,r.yg)(l.A,{value:"py1",label:"Javelin SDK",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell"},"pip install javelin-sdk\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-py"},'from javelin_sdk import (\n    JavelinClient,\n    Route\n)\n\nimport os\n\njavelin_api_key = os.getenv(\'JAVELIN_API_KEY\')\nllm_api_key = os.getenv("OPENAI_API_KEY")\n\n# create javelin client\nclient = JavelinClient(javelin_api_key=javelin_api_key, \n                       llm_api_key=llm_api_key)\n\n# route name to get is {routename} e.g., sampleroute1\nquery_data = {\n    "messages": [ \n        {\n            "role": "system",\n            "content": "Hello, you are a helpful scientific assistant."\n        },\n        {\n            "role": "user",\n            "content": "What is the chemical composition of sugar?"\n        }\n    ],\n    "temperature": 0.8,\n}\n\n# now query the route, for async use `await client.aquery_route("sampleroute1", query_data)`\nresponse = client.query_route("sampleroute1", query_data)\nprint(response.model_dump_json(indent=2))\n\n'))),(0,r.yg)(l.A,{value:"py2",label:"OpenAI",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell"},"pip install openai\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-py"},'from openai import OpenAI\nimport os\n\njavelin_api_key = os.environ[\'JAVELIN_API_KEY\']\nllm_api_key = os.environ["OPENAI_API_KEY"]\n\n# Javelin Headers\njavelin_headers = {\n                    "x-api-key": javelin_api_key,       # Javelin API key from admin\n                     "x-javelin-route": "sampleroute1"  # Javelin route to use\n                  }\n\n# Create OpenAI Client\nclient = OpenAI(api_key=llm_api_key,\n                base_url="https://api.javelin.live/v1/query",\n                default_headers=javelin_headers)\n\n# Query the model\ncompletion = client.chat.completions.create(\n  model="gpt-3.5-turbo",\n  messages=[\n    {"role": "system", "content": "Hello, you are a helpful scientific assistant"},\n    {"role": "user", "content": "What is the chemical composition of sugar?"}\n  ])\n\nprint(completion.model_dump_json(indent=2))\n\n# Streaming Responses\nstream = client.chat.completions.create(\n    model="gpt-3.5-turbo",\n    messages=[\n      {"role": "system", "content": "Hello, you are a helpful scientific assistant."},\n      {"role": "user", "content": "What is the chemical composition of sugar?"}\n    ],\n    stream=True,\n)\n\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or "", end="")\n\n'))),(0,r.yg)(l.A,{value:"py3",label:"Azure OpenAI",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell\x3c!--"},"pip install openai\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-py"},'from openai import AzureOpenAI\nimport os\n\n# Javelin Headers\njavelin_api_key = os.environ[\'JAVELIN_API_KEY\']\nllm_api_key = os.environ["AZURE_OPENAI_API_KEY"]\n\njavelin_headers = {\n                    "x-api-key": javelin_api_key,     # Javelin API key from admin\n                    "x-javelin-route": "sampleroute1" # Javelin route to use\n                  }\n\nclient = AzureOpenAI(api_key=llm_api_key,\n                     base_url="https://api.javelin.live/v1/query",\n                     default_headers=javelin_headers,\n                     api_version="2023-07-01-preview")\n\ncompletion = client.chat.completions.create(\n  model="gpt-3.5-turbo",\n  messages=[\n    {"role": "system", "content": "Hello, you are a helpful scientific assistant."},\n    {"role": "user", "content": "What is the chemical composition of sugar?"}\n  ]\n)\n\nprint(completion.model_dump_json(indent=2))\n\n# Streaming Responses\nstream = client.chat.completions.create(\n    model="gpt-3.5-turbo",\n    messages=[\n      {"role": "system", "content": "Hello, you are a helpful scientific assistant."},\n      {"role": "user", "content": "What is the chemical composition of sugar?"}\n    ],\n    stream=True,\n)\n\nfor chunk in stream:\n  if chunk.choices:\n    print(chunk.choices[0].delta.content or "", end="")\n\n'))),(0,r.yg)(l.A,{value:"py4",label:"LangChain",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell\x3c!--"},"pip install langchain\npip install langchain-openai\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-py"},'from langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nimport os\n\njavelin_api_key = os.getenv(\'JAVELIN_API_KEY\')\nllm_api_key = os.getenv("OPENAI_API_KEY")\njavelin_headers = {\n                    "x-api-key": javelin_api_key,      # Javelin API key from admin\n                    "x-javelin-route": "sample_route1" # Javelin route to use\n                  }\n\nllm = ChatOpenAI(\n    openai_api_base="https://api.javelin.live/v1/query",\n    openai_api_key=llm_api_key,\n    model_kwargs={\n      "extra_headers": javelin_headers\n    },\n)\n\nprompt = ChatPromptTemplate.from_messages([\n    ("system", "Hello, you are a helpful scientific assistant."),\n    ("user", "{input}")\n])\n\noutput_parser = StrOutputParser()\n\nchain = prompt | llm | output_parser\n\nprint(chain.invoke({"input": "What is the chemical composition of sugar?"}))\n'))),(0,r.yg)(l.A,{value:"py5",label:"DSPy",mdxType:"TabItem"},(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Introduction:")," ",(0,r.yg)("a",{parentName:"p",href:"https://towardsdatascience.com/intro-to-dspy-goodbye-prompting-hello-programming-4ca1c6ce3eb9"},"DSPy: Goodbye Prompting, Hello Programming!"),(0,r.yg)("br",{parentName:"p"}),"\n",(0,r.yg)("strong",{parentName:"p"},"Documentation:")," ",(0,r.yg)("a",{parentName:"p",href:"https://dspy-docs.vercel.app/"},"DSPy Docs")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell\x3c!--"},"pip install dspy-ai\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-py"},'import dspy\nfrom dsp import LM\nimport os\nimport requests\n\n# Assuming the environment variables are set correctly\njavelin_api_key = os.getenv(\'JAVELIN_API_KEY\')\nllm_api_key = os.getenv("OPENAI_API_KEY")\n\nclass Javelin(LM):\n    def __init__(self, model, api_key):\n        self.model = model\n        self.api_key = api_key\n        self.provider = "default"\n        self.kwargs = { \n                    "temperature": 1.0, \n                    "max_tokens": 500, \n                    "top_p": 1.0, \n                    "frequency_penalty": 0.0, \n                    "presence_penalty": 0.0, \n                    "stop": None, \n                    "n": 1, \n                    "logprobs": None, \n                    "logit_bias": None,\n                    "stream": False\n        }\n\n        self.base_url = "https://api.javelin.live/v1/query/"\n        self.javelin_headers = {\n                    "Content-Type": "application/json",\n                    "Authorization": f"Bearer { api_key }",\n                    "x-javelin-route": "openai", # route name configured for OpenAI\n                    "x-api-key": javelin_api_key,\n        }\n\n        self.history = []\n\n    def basic_request(self, prompt: str, **kwargs):\n        headers = self.javelin_headers\n\n        data = {\n            **kwargs,\n            "model": self.model,\n            "messages": [\n                {"role": "user", "content": prompt}\n            ]\n        }\n\n        response = requests.post(self.base_url, headers=headers, json=data)\n        response = response.json()\n\n        self.history.append({\n            "prompt": prompt,\n            "response": response,\n            "kwargs": kwargs,\n        })\n        return response\n\n    def __call__(self, prompt, only_completed=True, return_sorted=False, **kwargs):\n        response = self.request(prompt, **kwargs)\n        if \'choices\' in response and len(response[\'choices\']) > 0:\n            first_choice_content = response[\'choices\'][0][\'message\'][\'content\']\n            completions = [first_choice_content]\n            return completions\n        else:\n            return ["No response found."]\n\njavelin = Javelin(model="gpt-4-1106-preview", api_key=llm_api_key)\ndspy.configure(lm=javelin)\n\n# Define a module (ChainOfThought) and assign it a signature (return an answer, given a question).\nqa = dspy.ChainOfThought(\'question -> answer\')\nresponse = qa(question="You have 3 baskets. The first basket has twice as many apples as the second basket. The third basket has 3 fewer apples than the first basket. If you have a total of 27 apples, how many apples are in each basket?")\nprint(response)\n'))),(0,r.yg)(l.A,{value:"py6",label:"...",mdxType:"TabItem"},(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("a",{parentName:"p",href:"https://docs.datastax.com/en/ragstack/docs/index.html"},"DataStax RAGStack"))),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("a",{parentName:"p",href:"https://jxnl.github.io/instructor/"},"Instructor, Generating Structure from LLMs"))),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("a",{parentName:"p",href:"https://microsoft.github.io/promptflow/index.html#"},"Microsoft Prompt flow")))))),(0,r.yg)("h3",{id:"javascripttypescript"},"JavaScript/TypeScript"),(0,r.yg)(o.A,{mdxType:"Tabs"},(0,r.yg)(l.A,{value:"js1",label:"OpenAI",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell"},"npm install openai\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-js"},'import OpenAI from "openai";\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n  baseURL: "https://api.javelin.live/v1/query",\n  defaultHeaders: {\n    "x-api-key": `${process.env.JAVELIN_API_KEY}`,\n    "x-javelin-route": "sample_route1",\n  },\n});\n\nasync function main() {\n  const completion = await openai.chat.completions.create({\n    messages: [{ role: "system", content: "You are a helpful assistant." }],\n    model: "gpt-3.5-turbo",\n  });\n\n  console.log(completion.choices[0]);\n}\n\nmain();\n\n'))),(0,r.yg)(l.A,{value:"js2",label:"Langchain",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell"},"npm install @langchain/openai\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-js"},'\nimport { ChatOpenAI } from \'@langchain/openai\';\n\nconst llm = new ChatOpenAI({\n    openAIApiKey: process.env.OPENAI_API_KEY,\n    configuration: {\n        basePath: "https://api.javelin.live/v1/query",\n        defaultHeaders: {\n          "x-api-key": `${process.env.JAVELIN_API_KEY}`,\n          "x-javelin-route": "sample_route1",\n        },\n    },\n});\n\nasync function main() {\n  const response = await llm.invoke("tell me a joke?");\n  console.log(response);\n}\n\nmain();\n\n'))),(0,r.yg)(l.A,{value:"js3",label:"...",mdxType:"TabItem"},(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("a",{parentName:"p",href:"https://sdk.vercel.ai/docs"},"Vercel AI SDK")," -> ",(0,r.yg)("a",{parentName:"p",href:"https://vercel.com/blog/ai-integrations"},"AI Integrations on Vercel"))),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("a",{parentName:"p",href:"https://github.com/huggingface/chat-ui"},"Hugging Face ChatUI")," -> ",(0,r.yg)("a",{parentName:"p",href:"https://github.com/huggingface/chat-ui?tab=readme-ov-file#running-your-own-models-using-a-custom-endpoint"},"Running using a custom endpoint")))),(0,r.yg)("p",null,"We have worked on the integrations. Please contact: ",(0,r.yg)("a",{parentName:"p",href:"mailto:support@getjavelin.io"},"support@getjavelin.io")," if you would like to use this feature."))))}y.isMDXComponent=!0}}]);