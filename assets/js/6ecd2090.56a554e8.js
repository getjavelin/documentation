"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[7152],{6634:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>s,default:()=>u,frontMatter:()=>a,metadata:()=>c,toc:()=>l});var r=t(4848),i=t(5680);const a={},s="Semantic Caching",c={id:"javelin-core/features/caching",title:"Semantic Caching",description:"In the realm of AI-driven applications, ensuring efficient cost structures and swift responses is vital. Javelin\u2019s innovative semantic cache offers a balanced solution that significantly reduces costs while dramatically enhancing performance. Unlike traditional caches that rely on exact data matches, the semantic cache understands the underlying meaning of a request. This means even if a user\u2019s query isn't an exact match to a previously stored query, the cache can recognize its semantic similarity and fetch the relevant response.",source:"@site/docs/javelin-core/features/caching.md",sourceDirName:"javelin-core/features",slug:"/javelin-core/features/caching",permalink:"/docs/javelin-core/features/caching",draft:!1,unlisted:!1,editUrl:"https://github.com/getjavelin/documentation/tree/main/docs/javelin-core/features/caching.md",tags:[],version:"current",frontMatter:{},sidebar:"someSidebar",previous:{title:"Load Balancing",permalink:"/docs/javelin-core/features/loadbalancing"},next:{title:"Automatic & Intelligent LLM Selection",permalink:"/docs/javelin-core/features/automaticllm"}},o={},l=[{value:"Significant Cost Savings",id:"significant-cost-savings",level:3},{value:"Improved Response Time",id:"improved-response-time",level:3},{value:"Consistent Performance",id:"consistent-performance",level:3}];function d(e){const n={h1:"h1",h3:"h3",p:"p",strong:"strong",...(0,i.RP)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"semantic-caching",children:"Semantic Caching"}),"\n",(0,r.jsx)(n.p,{children:"In the realm of AI-driven applications, ensuring efficient cost structures and swift responses is vital. Javelin\u2019s innovative semantic cache offers a balanced solution that significantly reduces costs while dramatically enhancing performance. Unlike traditional caches that rely on exact data matches, the semantic cache understands the underlying meaning of a request. This means even if a user\u2019s query isn't an exact match to a previously stored query, the cache can recognize its semantic similarity and fetch the relevant response."}),"\n",(0,r.jsx)(n.h3,{id:"significant-cost-savings",children:"Significant Cost Savings"}),"\n",(0,r.jsx)(n.p,{children:"With Javelin's cache, LLM-related expenses can be drastically minimized:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"10x Cost Reduction:"})," By reducing the need to repeatedly query the LLM for familiar requests, Javelin's cache can diminish associated costs by up to tenfold. For applications that field frequently repeated or semantically similar queries, the savings can be substantial."]}),"\n",(0,r.jsx)(n.h3,{id:"improved-response-time",children:"Improved Response Time"}),"\n",(0,r.jsx)(n.p,{children:"Speed is paramount in the user experience:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"100x Faster Responses:"})," By serving answers directly from the cache, response times can be accelerated by up to a hundredfold, ensuring users receive answers almost instantaneously."]}),"\n",(0,r.jsx)(n.h3,{id:"consistent-performance",children:"Consistent Performance"}),"\n",(0,r.jsx)(n.p,{children:"LLMs can occasionally experience delays during peak times:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"High Availability:"})," By relying on the cache, applications can maintain consistent performance levels, even during times when LLMs face high demand. This ensures a uniform user experience regardless of external factors."]})]})}function u(e={}){const{wrapper:n}={...(0,i.RP)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},5680:(e,n,t)=>{t.d(n,{RP:()=>l});var r=t(6540);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function s(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function c(e,n){if(null==e)return{};var t,r,i=function(e,n){if(null==e)return{};var t,r,i={},a=Object.keys(e);for(r=0;r<a.length;r++)t=a[r],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)t=a[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var o=r.createContext({}),l=function(e){var n=r.useContext(o),t=n;return e&&(t="function"==typeof e?e(n):s(s({},n),e)),t},d={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},u=r.forwardRef((function(e,n){var t=e.components,i=e.mdxType,a=e.originalType,o=e.parentName,u=c(e,["components","mdxType","originalType","parentName"]),h=l(t),p=i,m=h["".concat(o,".").concat(p)]||h[p]||d[p]||a;return t?r.createElement(m,s(s({ref:n},u),{},{components:t})):r.createElement(m,s({ref:n},u))}));u.displayName="MDXCreateElement"}}]);