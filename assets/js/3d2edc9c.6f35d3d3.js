"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[6574],{82223:(e,n,t)=>{t.d(n,{A:()=>i});t(96540);var o=t(18215);const a={tabItem:"tabItem_Ymn6"};var r=t(74848);function i(e){let{children:n,hidden:t,className:i}=e;return(0,r.jsx)("div",{role:"tabpanel",className:(0,o.A)(a.tabItem,i),hidden:t,children:n})}},72206:(e,n,t)=>{t.d(n,{A:()=>I});var o=t(96540),a=t(18215),r=t(80052),i=t(56347),s=t(35793),l=t(99025),p=t(4430),c=t(44148);function d(e){return o.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,o.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function u(e){const{values:n,children:t}=e;return(0,o.useMemo)((()=>{const e=n??function(e){return d(e).map((e=>{let{props:{value:n,label:t,attributes:o,default:a}}=e;return{value:n,label:t,attributes:o,default:a}}))}(t);return function(e){const n=(0,p.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function m(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function h(e){let{queryString:n=!1,groupId:t}=e;const a=(0,i.W6)(),r=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,l.aZ)(r),(0,o.useCallback)((e=>{if(!r)return;const n=new URLSearchParams(a.location.search);n.set(r,e),a.replace({...a.location,search:n.toString()})}),[r,a])]}function g(e){const{defaultValue:n,queryString:t=!1,groupId:a}=e,r=u(e),[i,l]=(0,o.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!m({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const o=t.find((e=>e.default))??t[0];if(!o)throw new Error("Unexpected error: 0 tabValues");return o.value}({defaultValue:n,tabValues:r}))),[p,d]=h({queryString:t,groupId:a}),[g,v]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[a,r]=(0,c.Dv)(t);return[a,(0,o.useCallback)((e=>{t&&r.set(e)}),[t,r])]}({groupId:a}),_=(()=>{const e=p??g;return m({value:e,tabValues:r})?e:null})();(0,s.A)((()=>{_&&l(_)}),[_]);return{selectedValue:i,selectValue:(0,o.useCallback)((e=>{if(!m({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);l(e),d(e),v(e)}),[d,v,r]),tabValues:r}}var v=t(75251);const _={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var f=t(74848);function y(e){let{className:n,block:t,selectedValue:o,selectValue:i,tabValues:s}=e;const l=[],{blockElementScrollPositionUntilNextRender:p}=(0,r.a_)(),c=e=>{const n=e.currentTarget,t=l.indexOf(n),a=s[t].value;a!==o&&(p(n),i(a))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=l.indexOf(e.currentTarget)+1;n=l[t]??l[0];break}case"ArrowLeft":{const t=l.indexOf(e.currentTarget)-1;n=l[t]??l[l.length-1];break}}n?.focus()};return(0,f.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":t},n),children:s.map((e=>{let{value:n,label:t,attributes:r}=e;return(0,f.jsx)("li",{role:"tab",tabIndex:o===n?0:-1,"aria-selected":o===n,ref:e=>{l.push(e)},onKeyDown:d,onClick:c,...r,className:(0,a.A)("tabs__item",_.tabItem,r?.className,{"tabs__item--active":o===n}),children:t??n},n)}))})}function b(e){let{lazy:n,children:t,selectedValue:r}=e;const i=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=i.find((e=>e.props.value===r));return e?(0,o.cloneElement)(e,{className:(0,a.A)("margin-top--md",e.props.className)}):null}return(0,f.jsx)("div",{className:"margin-top--md",children:i.map(((e,n)=>(0,o.cloneElement)(e,{key:n,hidden:e.props.value!==r})))})}function k(e){const n=g(e);return(0,f.jsxs)("div",{className:(0,a.A)("tabs-container",_.tabList),children:[(0,f.jsx)(y,{...n,...e}),(0,f.jsx)(b,{...n,...e})]})}function I(e){const n=(0,v.A)();return(0,f.jsx)(k,{...e,children:d(e.children)},String(n))}},34366:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>i,metadata:()=>o,toc:()=>p});const o=JSON.parse('{"id":"javelin-core/integration","title":"Applications","description":"Its easy to integrate applications that leverage LLMs with Javelin. We have made it easy to seamlessly connect your applications to route all LLM traffic through Javelin with minimal code changes.","source":"@site/docs/javelin-core/integration.md","sourceDirName":"javelin-core","slug":"/javelin-core/integration","permalink":"/docs/javelin-core/integration","draft":false,"unlisted":false,"editUrl":"https://github.com/getjavelin/documentation/tree/main/docs/javelin-core/integration.md","tags":[],"version":"current","frontMatter":{},"sidebar":"someSidebar","previous":{"title":"Supported Language Models","permalink":"/docs/javelin-core/supported-llms"},"next":{"title":"Overview","permalink":"/docs/javelin-webapp/threat-alerts/overview"}}');var a=t(74848),r=t(28453);t(72206),t(82223),t(3320);const i={},s="Applications",l={},p=[{value:"Leveraging the Javelin Platform",id:"leveraging-the-javelin-platform",level:2},{value:"Unified Endpoints",id:"unified-endpoints",level:2},{value:"Endpoint Breakdown",id:"endpoint-breakdown",level:2},{value:"1. OpenAI-Compatible Endpoints",id:"1-openai-compatible-endpoints",level:3},{value:"Endpoints",id:"endpoints",level:4},{value:"Example Usage",id:"example-usage",level:4}];function c(e){const n={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"applications",children:"Applications"})}),"\n",(0,a.jsx)(n.p,{children:"Its easy to integrate applications that leverage LLMs with Javelin. We have made it easy to seamlessly connect your applications to route all LLM traffic through Javelin with minimal code changes."}),"\n",(0,a.jsx)(n.h2,{id:"leveraging-the-javelin-platform",children:"Leveraging the Javelin Platform"}),"\n",(0,a.jsxs)(n.p,{children:["The core usage of Javelin is to define routes, and then to define what to do at each route. Rather than having your LLM Applications (like Co-Pilot apps etc.,) individually & directly point to the LLM Vendor & Model (like OpenAI, Gemini etc.,), configure the provider/model endpoint to be your Javelin endpoint. This ensures that all applications that leverage AI Models will route their requests through the gateway. Javelin supports all the ",(0,a.jsx)(n.a,{href:"supported-llms",children:"latest models and providers"}),", so you don't have to make any changes to your application or how requests to models are sent."]}),"\n",(0,a.jsxs)(n.p,{children:["See ",(0,a.jsx)(n.a,{href:"routeconfiguration",children:"Javelin Configuration"})," section, for details on how to setup routes on the gateway to different models and providers."]}),"\n",(0,a.jsxs)(n.p,{children:["See ",(0,a.jsx)(n.a,{href:"../javelin-python/quickstart",children:"Python SDK"})," for details on how you can easily embed this within your AI Apps."]}),"\n",(0,a.jsx)(n.h2,{id:"unified-endpoints",children:"Unified Endpoints"}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.strong,{children:"Unified Endpoints"})," provide a consistent API interface that abstracts the provider-specific details of various AI services. Whether you are interfacing with an OpenAI-compatible service, an Azure OpenAI deployment, or an AWS Bedrock API, these endpoints enable you to use a standardized request/response format. This documentation explains the available endpoints, their purpose, and usage examples."]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Single Entry Points"}),": Instead of routing to different URLs for each provider, you call these \u201cunified\u201d endpoints with specific route parameters or path segments (e.g., ",(0,a.jsx)(n.code,{children:"/completions"}),", ",(0,a.jsx)(n.code,{children:"/chat/completions"}),", ",(0,a.jsx)(n.code,{children:"/embeddings"}),", or ",(0,a.jsx)(n.code,{children:"deployments/{deployment}/completions"})," in the case of Azure)."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Provider-Agnostic Handling"}),": A common handler (e.g., ",(0,a.jsx)(n.code,{children:"queryHandler(appState)"}),") receives each request and delegates it to the appropriate provider logic based on URL parameters like ",(0,a.jsx)(n.code,{children:"providername"})," or ",(0,a.jsx)(n.code,{children:"deployment"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Consistent Request/Response Shapes"}),": All requests follow a uniform structure (for example, a JSON object with a ",(0,a.jsx)(n.code,{children:"prompt"}),", ",(0,a.jsx)(n.code,{children:"messages"}),", or ",(0,a.jsx)(n.code,{children:"input"})," for embeddings). The service then translates it to each provider\u2019s specific API format as needed."]}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"endpoint-breakdown",children:"Endpoint Breakdown"}),"\n",(0,a.jsx)(n.h3,{id:"1-openai-compatible-endpoints",children:"1. OpenAI-Compatible Endpoints"}),"\n",(0,a.jsx)(n.p,{children:"These endpoints mirror the standard OpenAI API methods. They allow you to perform common AI tasks such as generating text completions, handling chat-based requests, or producing embeddings."}),"\n",(0,a.jsx)(n.h4,{id:"endpoints",children:"Endpoints"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsxs)(n.strong,{children:["POST ",(0,a.jsx)(n.code,{children:"/{providername}/completions"})]}),(0,a.jsx)(n.br,{}),"\n","Request text completions from the provider.",(0,a.jsx)(n.br,{}),"\n",(0,a.jsx)(n.strong,{children:"Path Parameter:"})]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"providername"}),": Identifier for the OpenAI-compatible provider."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsxs)(n.strong,{children:["POST ",(0,a.jsx)(n.code,{children:"/{providername}/chat/completions"})]}),(0,a.jsx)(n.br,{}),"\n","Request chat-based completions (ideal for conversational interfaces).",(0,a.jsx)(n.br,{}),"\n",(0,a.jsx)(n.strong,{children:"Path Parameter:"})]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"providername"}),": Identifier for the provider."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsxs)(n.strong,{children:["POST ",(0,a.jsx)(n.code,{children:"/{providername}/embeddings"})]}),(0,a.jsx)(n.br,{}),"\n","Generate embeddings for provided text data.",(0,a.jsx)(n.br,{}),"\n",(0,a.jsx)(n.strong,{children:"Path Parameter:"})]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"providername"}),": Identifier for the provider."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"example-usage",children:"Example Usage"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'curl -X POST "https://your-api-domain.com/openai/completions" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n        "prompt": "Once upon a time",\n        "max_tokens": 50\n      }\'\n\nReplace openai with the appropriate openai API compatible provider name like azure, mistral, deepseek etc. as required.\n\n### 2. Azure OpenAI API Endpoints\n\nFor providers using Azure\u2019s deployment model, endpoints include an additional parameter for deployment management.\n\n#### Endpoints\n\n- **POST `/{providername}/deployments/{deployment}/completions`**  \n  Request text completions from the provider.  \n  **Path Parameter:**  \n  - `providername`: The Azure OpenAI provider identifier.\n  - `deployment`: The deployment ID configured in Azure.\n\n- **POST `/{providername}/deployments/{deployment}/chat/completions`**  \n  Request chat-based completions (ideal for conversational interfaces).  \n  **Path Parameter:**  \n  - `providername`: The Azure OpenAI provider identifier.\n  - `deployment`: The deployment ID configured in Azure.\n\n- **POST `/{providername}/deployments/{deployment}/embeddings`**  \n  Generate embeddings for provided text data.  \n  **Path Parameter:**  \n  - `providername`: The Azure OpenAI provider identifier.\n  - `deployment`: The deployment ID configured in Azure.\n\n#### Example Usage\n\n```bash\ncurl -X POST "https://your-api-domain.com/azure/deployments/my-deployment/chat/completions" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n        "messages": [\n           {"role": "user", "content": "Tell me a story"}\n        ],\n        "max_tokens": 50\n      }\'\n\n### 3. AWS Bedrock API Endpoints\n\nFor AWS Bedrock\u2013style providers, the endpoints use a slightly different URL pattern to accommodate model versioning and extended routing.\n\n#### Endpoints\n\n- **POST `/model/{routename}/{apivariation}`**  \n  Route requests to a specific model and API variation.\n  **Path Parameter:**  \n  - `routename`: The model or route name (identifies a specific AWS Bedrock model).\n  - `apivariation`:  A parameter to indicate the API variation (Invoke", "Invoke-Stream", "Invoke-With-Response-Stream", "Converse", "Converse-Stream) or version.\n\n#### Example Usage\n\n```bash\ncurl -X POST "https://your-api-domain.com//model/anthropic.claude-3-sonnet-20240229-v1:0/invoke" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n        "input": "What is the capital of France?"\n      }\'\n\n### 4. Query Endpoints\n\nThese endpoints allow direct querying of predefined routes, bypassing provider-specific names when a generic and customizable route configuration is desired.\n\n#### Endpoints\n\n- **POST `/query/{routename}`**  \n  Execute a query against a specific route.\n  **Path Parameter:**  \n  - `routename`: The route with one or more models based on the configured policies and route configurations and return back a response\n\n###  Example Usage\n\n### REST API\n<Tabs>\n<TabItem value="curl" label="curl">\n\nFirst, create a route as shown in the [Create Route](../javelin-core/administration/createroute) section.\n\nOnce you have created a route, you can query it using the following curl command:\n\n<CodeBlock\n  language="bash">\n  {`curl \'https://api-dev.javelin.live/v1/query/your_route_name\' \\\\\n  -H \'Content-Type: application/json\' \\\\\n  -H \'Authorization: Bearer YOUR_OPENAI_API_KEY\' \\\\\n  -H \'x-api-key: YOUR_JAVELIN_API_KEY\' \\\\\n  --data-raw \'{\n    "model": "gpt-3.5-turbo",\n    "messages": [\n      {"role": "user", "content": "SANFRANCISCO is located in?"}\n    ],\n    "temperature": 0.8\n  }\'`}\n</CodeBlock>\n\nMake sure to replace `your_route_name`, `YOUR_OPENAI_API_KEY`, and `YOUR_JAVELIN_API_KEY` with your actual values.\n\n</TabItem>\n<TabItem value="python" label="Python Requests">\n\nFirst, create a route as shown in the [Create Route](../javelin-core/administration/createroute) section.\n\nOnce you have created a route, you can query it using Python requests:\n\n<CodeBlock\n  language="python">\n  {`import requests\nimport os\nimport dotenv\n\ndotenv.load_dotenv()\n\njavelin_api_key = os.getenv(\'JAVELIN_API_KEY\')\nopenai_api_key = os.getenv(\'OPENAI_API_KEY\')\nroute_name = \'your_route_name\'\n\nurl = f\'https://api-dev.javelin.live/v1/query/{route_name}\'\n\nheaders = {\n    \'Content-Type\': \'application/json\',\n    \'Authorization\': f\'Bearer {openai_api_key}\',\n    \'x-api-key\': javelin_api_key\n}\n\ndata = {\n    "model": "gpt-3.5-turbo",\n    "messages": [\n        {"role": "user", "content": "SANFRANCISCO is located in?"}\n    ],\n    "temperature": 0.8\n}\n\nresponse = requests.post(url, headers=headers, json=data)\n\nif response.status_code == 200:\n    print(response.json())\nelse:\n    print(f"Error: {response.status_code}, {response.text}")`}\n</CodeBlock>\n\nMake sure to replace `your_route_name` with your actual route name and set the `JAVELIN_API_KEY` and `OPENAI_API_KEY` environment variables.\n\n</TabItem>\n</Tabs>\n\n### Python\n<Tabs>\n<TabItem value="py1" label="Javelin SDK">\n\n<CodeBlock\n  language="python">\n  {`pip install javelin-sdk\n`}\n</CodeBlock>\n\n<CodeBlock\n  language="python"\n  title="Query Route with Javelin SDK"\n  showLineNumbers>\n  {`from javelin_sdk import JavelinClient, JavelinConfig, Route\nimport os\n\njavelin_api_key = os.getenv(\'JAVELIN_API_KEY\')\nllm_api_key = os.getenv("OPENAI_API_KEY")\n\n# Create Javelin configuration\nconfig = JavelinConfig(\n    base_url="https://api-dev.javelin.live",\n    javelin_api_key=javelin_api_key,\n    llm_api_key=llm_api_key\n)\n\n# Create Javelin client\nclient = JavelinClient(config)\n\n# Route name to get is {routename} e.g., sampleroute1\nquery_data = {\n    "messages": [ \n        {\n            "role": "system",\n            "content": "Hello, you are a helpful scientific assistant."\n        },\n        {\n            "role": "user",\n            "content": "What is the chemical composition of sugar?"\n        }\n    ],\n    "temperature": 0.8\n}\n\n# Now query the route, for async use \'await client.aquery_route("sampleroute1", query_data)\'\nresponse = client.query_route("sampleroute1", query_data)\nprint(response.model_dump_json(indent=2))`}\n</CodeBlock>\n\n</TabItem>\n\n<TabItem value="py2" label="OpenAI">\n\n<CodeBlock\n  language="python">\n  {`pip install openai\n`}\n</CodeBlock>\n\n<CodeBlock\n  language="python"\n  title="Query and Stream Responses with OpenAI"\n  showLineNumbers>\n  {`from openai import OpenAI\nimport os\n\njavelin_api_key = os.environ[\'JAVELIN_API_KEY\']\nllm_api_key = os.environ["OPENAI_API_KEY"]\n\n## Javelin Headers\n# Define Javelin headers with the API key\nconfig = JavelinConfig(\n  javelin_api_key=javelin_api_key,\n)\n\n# Define the Javelin route as a variable\njavelin_route = "sampleroute1"  # Define your universal route\n\nclient = JavelinClient(config)\n\n# Register the OpenAI client with Javelin using the route name\nclient.register_openai(openai_client, route_name=javelin_route)\n\n\n# Query the model\n# --- Call OpenAI Endpoints ---\n\nprint("OpenAI: 1 - Chat completions")\nchat_completions = openai_client.chat.completions.create(\n    model="gpt-3.5-turbo",\n    messages=[{"role": "user", "content": "What is machine learning?"}],\n)\nprint(completion.model_dump_json(indent=2))\n\n# Streaming Responses\nstream = openai_client.chat.completions.create(\n    messages=[\n        {"role": "user", "content": "Say this is a test"}\n    ],\n    model="gpt-4o",\n    stream=True,\n)\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or "", end="")`}\n</CodeBlock>\n\n\n</TabItem>\n\n<TabItem value="py3" label="Azure OpenAI">\n\n<CodeBlock\n  language="shell">\n  {`pip install openai`}\n</CodeBlock>\n\n<CodeBlock\n  language="python"\n  title="Query and Stream Responses with AzureOpenAI"\n  showLineNumbers>\n  {`from openai import AzureOpenAI\nimport os\n\n# Javelin Headers\njavelin_headers = {\n    "x-api-key": javelin_api_key  # Javelin API key from admin\n}\n\n# Define the Javelin route as a variable\njavelin_route = "sampleroute1"  # Example route\n\nclient = JavelinClient(config) # Create Javelin Client\nclient.register_azureopenai(openai_client, route_name=javelin_route) # Register Azure OpenAI Client with Javelin\n\n# Create Azure OpenAI Client\nopenai_client = AzureOpenAI(\n    api_version="2023-07-01-preview",\n    azure_endpoint="https://javelinpreview.openai.azure.com", # Azure Endpoint\n    api_key=azure_openai_api_key\n)\n\n\ncompletion = openai_client.chat.completions.create(\n    model="gpt-4",  # e.g. gpt-3.5-turbo\n    messages=[\n        {\n            "role": "user",\n            "content": "How do I output all files in a directory using Python?",\n        },\n    ],\n)\n\nprint(completion.model_dump_json(indent=2))\n\n# Streaming Responses\nstream = openai_client.chat.completions.create(\n    model="gpt-3.5-turbo",\n    messages=[\n      {"role": "system", "content": "Hello, you are a helpful scientific assistant."},\n      {"role": "user", "content": "What is the chemical composition of sugar?"}\n    ],\n    stream=True\n)\n\nfor chunk in stream:\n  if chunk.choices:\n    print(chunk.choices[0].delta.content or "", end="")`}\n</CodeBlock>\n\n\n</TabItem>\n\n<TabItem value="py4" label="LangChain">\n\n<CodeBlock\n  language="shell">\n  {`pip install langchain\npip install langchain-openai`}\n</CodeBlock>\n\n\n<CodeBlock\n  language="python"\n  title="LangChain with OpenAI Example"\n  showLineNumbers>\n  {`from langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nimport os\n\n# Retrieve API keys from environment variables\njavelin_api_key = os.getenv(\'JAVELIN_API_KEY\')\nllm_api_key = os.getenv("OPENAI_API_KEY")\n\nmodel_choice = "gpt-3.5-turbo"  # For example, change to "gpt-4"\n\n# Define the Javelin route as a variable\nroute_name = "sampleroute1"\n\n# Define Javelin headers with the API key\njavelin_headers = {\n    "x-api-key": javelin_api_key  # Javelin API key from admin\n}\n\nllm = ChatOpenAI(\n    openai_api_key=openai_api_key,\n    openai_api_base="https://api-dev.javelin.live/v1/openai",\n    default_headers={\n        "x-api-key": javelin_api_key,\n        "x-javelin-route": route_name,\n        "x-javelin-provider": "https://api.openai.com/v1",\n        "x-javelin-model":model_choice\n        \n    }\n)\n\n\n# Define a simple prompt template\nprompt = ChatPromptTemplate.from_messages([\n    ("system", "You are a helpful assistant."),\n    ("user", "{input}")\n])\n\n# Use a simple output parser (string output)\noutput_parser = StrOutputParser()\n\n# Create the processing chain (prompt -> LLM -> parser)\nchain = prompt | llm | output_parser\n\ndef ask_question(question: str) -> str:\n    return chain.invoke({"input": question})\n\n# Example usage:\nif __name__ == "__main__":\n    question = "What is the chemical composition of water?"\n    answer = ask_question(question)\n    print("Answer:", answer)\n\n\n`}\n</CodeBlock>\n\n</TabItem>\n\n<TabItem value="py7" label="OpenAI-Compatible Query Example">\n<CodeBlock\n  language="python"\n  title="OpenAI-Compatible Model Adapters Example"\n  showLineNumbers>\n  {`\n#This example demonstrates how Javelin uses OpenAI\'s schema as a standardized interface for different LLM providers. \n#By adopting OpenAI\'s widely-used request/response format, Javelin enables seamless integration with various LLM providers \n#(like Anthropic, Bedrock, Mistral, etc.) while maintaining a consistent API structure. This allows developers to use the \n#same code pattern regardless of the underlying model provider, with Javelin handling the necessary translations and adaptations behind the scenes.\n\nfrom javelin_sdk import JavelinClient, JavelinConfig\nimport os\nfrom typing import Dict, Any\nimport json\n\n# Helper function to pretty print responses\ndef print_response(provider: str, response: Dict[str, Any]) -> None:\n    print(f"\\n=== Response from {provider} ===")\n    print(json.dumps(response, indent=2))\n\n# Setup client configuration\nconfig = JavelinConfig(\n    base_url="https://api-dev.javelin.live",\n    javelin_api_key=os.getenv(\'JAVELIN_API_KEY\'),\n    llm_api_key=os.getenv(\'OPENAI_API_KEY\')\n)\nclient = JavelinClient(config)\n\n# Example messages in OpenAI format\nmessages = [\n    {"role": "system", "content": "You are a helpful assistant."},\n    {"role": "user", "content": "What are the three primary colors?"}\n]\n\n# 1. Query OpenAI route\ntry:\n    openai_response = client.chat.completions.create(\n        route="openai_route",  # Route configured for OpenAI\n        messages=messages,\n        temperature=0.7,\n        max_tokens=150\n    )\n    print_response("OpenAI", openai_response)\nexcept Exception as e:\n    print(f"OpenAI query failed: {str(e)}")\n    \n=== Response from OpenAI ===\n"""\n{\n  "id": "chatcmpl-123abc",\n  "object": "chat.completion",\n  "created": 1677858242,\n  "model": "gpt-3.5-turbo",\n  "usage": {\n    "prompt_tokens": 42,\n    "completion_tokens": 38,\n    "total_tokens": 80\n  },\n  "choices": [\n    {\n      "message": {\n        "role": "assistant",\n        "content": "The three primary colors are red, blue, and yellow."\n      },\n      "finish_reason": "stop",\n      "index": 0\n    }\n  ]\n}\n"""\n# 2. Query Bedrock route (using same OpenAI format)\ntry:\n    bedrock_response = client.chat.completions.create(\n        route="bedrock_route",  # Route configured for Bedrock\n        messages=messages,\n        temperature=0.7,\n        max_tokens=150\n    )\n    print_response("Bedrock", bedrock_response)\nexcept Exception as e:\n    print(f"Bedrock query failed: {str(e)}")\n"""\n=== Response from Bedrock ===\n{\n  "id": "bedrock-123xyz",\n  "object": "chat.completion",\n  "created": 1677858243,\n  "model": "anthropic.claude-v2",\n  "usage": {\n    "prompt_tokens": 42,\n    "completion_tokens": 41,\n    "total_tokens": 83\n  },\n  "choices": [\n    {\n      "message": {\n        "role": "assistant",\n        "content": "The three primary colors are red, blue, and yellow. These colors cannot be created by mixing other colors together."\n      },\n      "finish_reason": "stop",\n      "index": 0\n    }\n  ]\n}\n"""\n\n# Example using text completions with Llama\ntry:\n    llama_response = client.completions.create(\n        route="bedrockllama",  # Route configured for Bedrock Llama\n        prompt="Write a haiku about programming:",\n        max_tokens=50,\n        temperature=0.7,\n        top_p=0.9,\n    )\n    print("=== Llama Text Completion Response ===")\n    pretty_print(llama_response)\nexcept Exception as e:\n    print(f"Llama query failed: {str(e)}")\n\n"""\n=== Llama Text Completion Response ===\n{\n  "id": "bedrock-comp-123xyz",\n  "object": "text_completion",\n  "created": 1677858244,\n  "model": "meta.llama2-70b",\n  "choices": [\n    {\n      "text": "Code flows like water\\\\nBugs crawl through silent errors\\\\nDebugger saves all",\n      "index": 0,\n      "finish_reason": "stop"\n    }\n  ],\n  "usage": {\n    "prompt_tokens": 6,\n    "completion_tokens": 15,\n    "total_tokens": 21\n  }\n}\n"""\n\n`}\n</CodeBlock>\n\n</TabItem>\n\n\n<TabItem value="py5" label="DSPy">\n\n**Introduction:** [DSPy: Goodbye Prompting, Hello Programming!](https://towardsdatascience.com/intro-to-dspy-goodbye-prompting-hello-programming-4ca1c6ce3eb9)  \n**Documentation:** [DSPy Docs](https://dspy-docs.vercel.app/)\n\n<CodeBlock\n  language="shell">\n  {`pip install dspy-ai`}\n</CodeBlock>\n\n\n<CodeBlock\n  language="py"\n  title="Using DSPY with Javelin"\n  showLineNumbers>\n  {`import dspy\nfrom dsp import LM\nimport os\nimport requests\n\n# Assuming the environment variables are set correctly\njavelin_api_key = os.getenv(\'JAVELIN_API_KEY\')\nllm_api_key = os.getenv("OPENAI_API_KEY")\n\nclass Javelin(LM):\n    def __init__(self, model, api_key):\n        self.model = model\n        self.api_key = api_key\n        self.provider = "default"\n        self.kwargs = { \n                    "temperature": 1.0, \n                    "max_tokens": 500, \n                    "top_p": 1.0, \n                    "frequency_penalty": 0.0, \n                    "presence_penalty": 0.0, \n                    "stop": None, \n                    "n": 1, \n                    "logprobs": None, \n                    "logit_bias": None,\n                    "stream": False\n        }\n\n        self.base_url = "https://api-dev.javelin.live/v1/query/your_route_name" # Set Javelin\'s API base URL for query\n        self.javelin_headers = {\n                    "Content-Type": "application/json",\n                    "Authorization": f"Bearer { api_key }",\n                    "x-api-key": javelin_api_key,\n        }\n\n        self.history = []\n\n    def basic_request(self, prompt: str, **kwargs):\n        headers = self.javelin_headers\n\n        data = {\n            **kwargs,\n            "model": self.model,\n            "messages": [\n                {"role": "user", "content": prompt}\n            ]\n        }\n\n        response = requests.post(self.base_url, headers=headers, json=data)\n        response = response.json()\n\n        self.history.append({\n            "prompt": prompt,\n            "response": response,\n            "kwargs": kwargs,\n        })\n        return response\n\n    def __call__(self, prompt, only_completed=True, return_sorted=False, **kwargs):\n        response = self.request(prompt, **kwargs)\n        if \'choices\' in response and len(response[\'choices\']) > 0:\n            first_choice_content = response[\'choices\'][0][\'message\'][\'content\']\n            completions = [first_choice_content]\n            return completions\n        else:\n            return ["No response found."]\n\njavelin = Javelin(model="gpt-4-1106-preview", api_key=llm_api_key)\ndspy.configure(lm=javelin)\n\n# Define a module (ChainOfThought) and assign it a signature (return an answer, given a question).\nqa = dspy.ChainOfThought(\'question -> answer\')\nresponse = qa(question="You have 3 baskets. The first basket has twice as many apples as the second basket. The third basket has 3 fewer apples than the first basket. If you have a total of 27 apples, how many apples are in each basket?")\nprint(response)`}\n</CodeBlock>\n\n\n</TabItem>\n\n<TabItem value="py6" label="Bedrock">\n\n<CodeBlock\n  language="shell">\n  {`pip install boto3`}\n</CodeBlock>\n\n<CodeBlock\n  language="python"\n  title="AWS Bedrock Integration Example - Boto3"\n  showLineNumbers>\n  {`import boto3\nimport json\nfrom javelin_sdk import (\n    JavelinClient,\n    JavelinConfig,\n)\n\n# Configure boto3 bedrock-runtime service client\nbedrock_runtime_client = boto3.client(\n    service_name="bedrock-runtime",\n    region_name="us-east-1"\n)\n\n# Configure boto3 bedrock service client\nbedrock_client = boto3.client(\n    service_name="bedrock",\n    region_name="us-east-1"\n)\n\n\n# Initialize Javelin Client\nconfig = JavelinConfig(\n    base_url=os.getenv(\'JAVELIN_BASE_URL\'),\n    javelin_api_key=os.getenv(\'JAVELIN_API_KEY\')\n)\nclient = JavelinClient(config)\n\n# Passing bedrock_client is recommended for optimal error handling\n# and request management, though it remains optional.\nclient.register_bedrock(\n  bedrock_runtime_client=bedrock_runtime_client, \n  bedrock_client=bedrock_client, \n  route_name="bedrock" # Universal route for the Amazon Bedrock models\n)\n\n# Example using Claude model via Bedrock Runtime\nresponse = bedrock_runtime_client.invoke_model(\n    modelId="anthropic.claude-v2:1",\n    body=json.dumps({\n        "anthropic_version": "bedrock-2023-05-31",\n        "max_tokens": 100,\n        "messages": [\n            {\n                "content": "What is machine learning?",\n                "role": "user"\n            }\n        ]\n    }),\n    contentType="application/json"\n)\nresponse_body = json.loads(response["body"].read())\nprint(f"Invoke Response: {json.dumps(response_body, indent=2)}")\n\n`}\n</CodeBlock>\n\nLearn more about how to setup Universal Bedrock routes to use this example [here](../javelin-core/administration/createbedrockroutes).\n\n\n\n<CodeBlock\n  language="python"\n  title="AWS Bedrock Integration Example - LangChain"\n  showLineNumbers>\n  {`# Example using Langchain \n\nfrom langchain_community.llms.bedrock import Bedrock as BedrockLLM\n\nllm = BedrockLLM(\n    client=bedrock_runtime_client,\n    model_id="anthropic.claude-v2:1",\n    model_kwargs={\n        "max_tokens_to_sample": 256,\n        "temperature": 0.7,\n    }\n)\n\nstream_generator = llm.stream("What is machine learning?")\nfor chunk in stream_generator:\n    print(chunk, end=\'\', flush=True)\n\n`}\n</CodeBlock>\n\nLearn more about how to setup Universal Bedrock routes to use this example [here](../javelin-core/administration/createbedrockroutes).\n\n\n</TabItem>\n\n<TabItem value="py8" label="...">\n\n- [LlamaIndex](https://www.llamaindex.ai/open-source)\n\n- [DataStax RAGStack](https://docs.datastax.com/en/ragstack/docs/index.html)\n\n- [Instructor, Generating Structure from LLMs](https://jxnl.github.io/instructor/)\n\n- [Microsoft Prompt flow](https://microsoft.github.io/promptflow/index.html#)\n\n</TabItem>\n</Tabs>\n\n\n### JavaScript/TypeScript\n\n<Tabs>\n<TabItem value="js1" label="OpenAI">\n\n<CodeBlock\n  language="python">\n  {`npm install openai\n`}\n</CodeBlock>\n\n<CodeBlock\n  language="js"\n  title="OpenAI API Integration Example"\n  showLineNumbers>\n  {`import OpenAI from "openai";\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n  baseURL: "https://api-dev.javelin.live/v1/query",\n  defaultHeaders: {\n    "x-api-key": \\`\\${process.env.JAVELIN_API_KEY}\\`,\n    "x-javelin-route": "sample_route1",\n  },\n});\n\nasync function main() {\n  const completion = await openai.chat.completions.create({\n    messages: [{ role: "system", content: "You are a helpful assistant." }],\n    model: "gpt-3.5-turbo",\n  });\n\n  console.log(completion.choices[0]);\n}\n\nmain();`}\n</CodeBlock>\n\n\n</TabItem>\n\n<TabItem value="js2" label="Langchain">\n\n<CodeBlock\n  language="python">\n  {`npm install @langchain/openai\n`}\n</CodeBlock>\n\n<CodeBlock\n  language="js"\n  title="LangChain OpenAI Integration Example"\n  showLineNumbers>\n  {`import { ChatOpenAI } from \'@langchain/openai\';\n\nconst llm = new ChatOpenAI({\n    openAIApiKey: process.env.OPENAI_API_KEY,\n    configuration: {\n        basePath: "https://api-dev.javelin.live/v1/query",\n        defaultHeaders: {\n          "x-api-key": \\`\\${process.env.JAVELIN_API_KEY}\\`,\n          "x-javelin-route": "sample_route1",\n        },\n    },\n});\n\nasync function main() {\n  const response = await llm.invoke("tell me a joke?");\n  console.log(response);\n}\n\nmain();`}\n</CodeBlock>\n\n</TabItem>\n\n\n<TabItem value="js3" label="Bedrock">\n\n<CodeBlock\n  language="js"\n  title="AWS Bedrock Integration Example"\n  showLineNumbers>\n  {`import { BedrockRuntimeClient, InvokeModelCommand, InvokeModelWithResponseStreamCommand } from "@aws-sdk/client-bedrock-runtime";\n\nconst customHeaders = {\n  \'x-api-key\': JAVELIN_API_KEY\n};\n\nconst client = new BedrockRuntimeClient({\n  region: AWS_REGION,\n  // Use the javelin endpoint for bedrock\n  endpoint: JAVELIN_ENDPOINT,\n  credentials: {\n    accessKeyId: AWS_ACCESS_KEY_ID,\n    secretAccessKey: AWS_SECRET_ACCESS_KEY,\n  },\n});\n\n// Add custom headers via middleware\nclient.middlewareStack.add(\n  (next, context) => async (args) => {\n    args.request.headers = {\n      ...args.request.headers,\n      ...customHeaders\n    };\n    return next(args);\n  },\n  {\n    step: "build"\n  }\n);\n\n\n\n// Query the model\nconst payload = {\n  anthropic_version: "bedrock-2023-05-31",\n  max_tokens: 1000,\n  messages: [\n    {\n      role: "user",\n      content: "What is machine learning?",\n    },\n  ],\n};\n\n\nconst command = new InvokeModelWithResponseStreamCommand({\n  contentType: "application/json",\n  body: JSON.stringify(payload),\n  "anthropic.claude-v2:1",\n});\n\nconst apiResponse = await client.send(command);\n\nfor await (const item of apiResponse.body) {\n  console.log(item);\n}\n\n`}\n</CodeBlock>\n\nLearn more about how to setup Bedrock routes to use these examples [here](../javelin-core/administration/createbedrockroutes).\n\n</TabItem>\n\n\n<TabItem value="js4" label="...">\n\n- [Vercel AI SDK](https://sdk.vercel.ai/docs) -> [AI Integrations on Vercel](https://vercel.com/blog/ai-integrations)\n\n- [Hugging Face ChatUI](https://github.com/huggingface/chat-ui) -> [Running using a custom endpoint](https://github.com/huggingface/chat-ui?tab=readme-ov-file#running-your-own-models-using-a-custom-endpoint)\n\nWe have worked on the integrations. Please contact: support@getjavelin.io if you would like to use this feature.\n\n</TabItem>\n\n</Tabs>\n\n\n\n'})})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}}}]);