"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[1780],{21945:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>s,default:()=>u,frontMatter:()=>a,metadata:()=>o,toc:()=>l});var i=n(74848),r=n(15680);const a={},s="Automatic & Intelligent LLM Selection",o={id:"javelin-core/features/automaticllm",title:"Automatic & Intelligent LLM Selection",description:"The digital landscape is vast and varied, necessitating AI solutions that can efficiently cater to diverse needs.",source:"@site/docs/javelin-core/features/automaticllm.md",sourceDirName:"javelin-core/features",slug:"/javelin-core/features/automaticllm",permalink:"/docs/javelin-core/features/automaticllm",draft:!1,unlisted:!1,editUrl:"https://github.com/getjavelin/documentation/tree/main/docs/javelin-core/features/automaticllm.md",tags:[],version:"current",frontMatter:{},sidebar:"someSidebar",previous:{title:"Semantic Caching",permalink:"/docs/javelin-core/features/caching"},next:{title:"Archiving",permalink:"/docs/javelin-core/features/auditarchive"}},c={},l=[{value:"Dynamic Model Allocation",id:"dynamic-model-allocation",level:2},{value:"Benefits",id:"benefits",level:2}];function d(e){const t={a:"a",h1:"h1",h2:"h2",p:"p",strong:"strong",...(0,r.RP)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.h1,{id:"automatic--intelligent-llm-selection",children:"Automatic & Intelligent LLM Selection"}),"\n",(0,i.jsx)(t.p,{children:"The digital landscape is vast and varied, necessitating AI solutions that can efficiently cater to diverse needs."}),"\n",(0,i.jsx)(t.p,{children:"With the extensive range of Large Language Models (LLMs) available, the question often arises:"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"Which model is best for a particular task?"})}),"\n",(0,i.jsx)(t.p,{children:"Javelin provides an answer by automating this choice, ensuring optimal performance based on the desired parameters."}),"\n",(0,i.jsx)(t.h2,{id:"dynamic-model-allocation",children:"Dynamic Model Allocation"}),"\n",(0,i.jsx)(t.p,{children:"Javelin's advanced algorithm evaluates the requirements of a task and dynamically selects the most suitable LLM:"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Cost-Efficient:"})," For projects with strict budgets, Javelin can prioritize models that deliver good results at a lower cost."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Precision-Focused:"})," If a task requires high accuracy, Javelin will allocate models known for their intricate analyses and detailed outputs."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Speed-Centric:"})," For applications needing rapid feedback, Javelin opts for models streamlined for quick processing without significantly compromising on accuracy."]}),"\n",(0,i.jsx)(t.h2,{id:"benefits",children:"Benefits"}),"\n",(0,i.jsx)(t.p,{children:"By automating model selection, Javelin removes the complexities involved in manual LLM selection:"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"No Guesswork:"})," Developers need not spend time assessing which LLM is optimal for each task. Javelin\u2019s intelligence handles it."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Seamless Integration:"})," Users interact with Javelin\u2019s interface, receiving results from the best LLM for the task without even realizing the complex selection process happening behind the scenes."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Dynamic Adjustments:"})," As tasks evolve and demands change, Javelin can dynamically adjust its model selection criteria, ensuring sustained optimal performance."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"User Preferences:"})," If a user has a specific preference or requirement, they can guide Javelin\u2019s selection process by defining certain parameters or setting priorities."]}),"\n",(0,i.jsxs)(t.p,{children:["Please contact: ",(0,i.jsx)(t.a,{href:"mailto:support@getjavelin.io",children:"support@getjavelin.io"})," if you would like to use this feature."]})]})}function u(e={}){const{wrapper:t}={...(0,r.RP)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},15680:(e,t,n)=>{n.d(t,{RP:()=>l});var i=n(96540);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,i,r=function(e,t){if(null==e)return{};var n,i,r={},a=Object.keys(e);for(i=0;i<a.length;i++)n=a[i],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(i=0;i<a.length;i++)n=a[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var c=i.createContext({}),l=function(e){var t=i.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},d={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},u=i.forwardRef((function(e,t){var n=e.components,r=e.mdxType,a=e.originalType,c=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),p=l(n),m=r,h=p["".concat(c,".").concat(m)]||p[m]||d[m]||a;return n?i.createElement(h,s(s({ref:t},u),{},{components:n})):i.createElement(h,s({ref:t},u))}));u.displayName="MDXCreateElement"}}]);