"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[4903],{5680:(e,t,n)=>{n.d(t,{xA:()=>l,yg:()=>g});var i=n(6540);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,i,r=function(e,t){if(null==e)return{};var n,i,r={},o=Object.keys(e);for(i=0;i<o.length;i++)n=o[i],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(i=0;i<o.length;i++)n=o[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var c=i.createContext({}),p=function(e){var t=i.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},l=function(e){var t=p(e.components);return i.createElement(c.Provider,{value:t},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},m=i.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,c=e.parentName,l=s(e,["components","mdxType","originalType","parentName"]),u=p(n),m=r,g=u["".concat(c,".").concat(m)]||u[m]||d[m]||o;return n?i.createElement(g,a(a({ref:t},l),{},{components:n})):i.createElement(g,a({ref:t},l))}));function g(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,a=new Array(o);a[0]=m;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[u]="string"==typeof e?e:r,a[1]=s;for(var p=2;p<o;p++)a[p]=n[p];return i.createElement.apply(null,a)}return i.createElement.apply(null,n)}m.displayName="MDXCreateElement"},3695:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>p});var i=n(8168),r=(n(6540),n(5680));const o={},a="Prompt Injection",s={unversionedId:"javelin-processors/promptinjection",id:"javelin-processors/promptinjection",title:"Prompt Injection",description:"Prompt injection is a sophisticated technique where malicious users craft inputs to manipulate the behavior of Large Language Models (LLMs) like GPT-3 or GPT-4, potentially causing them to generate undesired, biased, or harmful outputs. This manipulation poses significant security and ethical risks, particularly in open-ended applications where users have the freedom to input arbitrary text. To mitigate these risks, it's crucial to employ robust detection and prevention strategies.",source:"@site/docs/javelin-processors/promptinjection.md",sourceDirName:"javelin-processors",slug:"/javelin-processors/promptinjection",permalink:"/docs/javelin-processors/promptinjection",draft:!1,editUrl:"https://github.com/getjavelin/documentation/tree/main/docs/javelin-processors/promptinjection.md",tags:[],version:"current",frontMatter:{},sidebar:"someSidebar",previous:{title:"Overview",permalink:"/docs/javelin-processors/overview"},next:{title:"Data Protection",permalink:"/docs/javelin-processors/privacyredaction"}},c={},p=[{value:"Understanding Prompt Injection",id:"understanding-prompt-injection",level:2},{value:"Javelin Prompt Injection Processor",id:"javelin-prompt-injection-processor",level:2},{value:"How It Works",id:"how-it-works",level:3},{value:"Input Analysis:",id:"input-analysis",level:4},{value:"Evaluation:",id:"evaluation",level:4},{value:"Action and Feedback:",id:"action-and-feedback",level:4},{value:"Integrating with Prompt Injection services",id:"integrating-with-prompt-injection-services",level:2}],l={toc:p},u="wrapper";function d(e){let{components:t,...n}=e;return(0,r.yg)(u,(0,i.A)({},l,n,{components:t,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"prompt-injection"},"Prompt Injection"),(0,r.yg)("p",null,"Prompt injection is a sophisticated technique where malicious users craft inputs to manipulate the behavior of Large Language Models (LLMs) like GPT-3 or GPT-4, potentially causing them to generate undesired, biased, or harmful outputs. This manipulation poses significant security and ethical risks, particularly in open-ended applications where users have the freedom to input arbitrary text. To mitigate these risks, it's crucial to employ robust detection and prevention strategies."),(0,r.yg)("h2",{id:"understanding-prompt-injection"},"Understanding Prompt Injection"),(0,r.yg)("p",null,"Prompt injection attacks exploit the generative capabilities of LLMs by inserting specially crafted commands or sequences within the input text, aiming to alter the model's behavior. Examples include generating outputs that breach privacy, disseminate misinformation, or perform actions unintended by the application's designers."),(0,r.yg)("h2",{id:"javelin-prompt-injection-processor"},"Javelin Prompt Injection Processor"),(0,r.yg)("h3",{id:"how-it-works"},"How It Works"),(0,r.yg)("h4",{id:"input-analysis"},"Input Analysis:"),(0,r.yg)("p",null,"As users submit prompts to the system, Javelin Processors first analyze the text to identify any unusual patterns, sequences, or commands that could signify an injection attempt."),(0,r.yg)("h4",{id:"evaluation"},"Evaluation:"),(0,r.yg)("p",null,"Inputs flagged by Javelin Processors are further evaluated by specialized models, which assess the risk based on a comprehensive database of known injection techniques and behaviors."),(0,r.yg)("h4",{id:"action-and-feedback"},"Action and Feedback:"),(0,r.yg)("p",null,"Depending on the evaluation, actions can range from simply logging the attempt, alerting administrators, sanitizing the input to remove malicious content, or outright rejecting the input. Feedback can also be used to refine and update the detection models, ensuring they remain effective against evolving injection strategies."),(0,r.yg)("p",null,"Javelin's Prompt Injection processors serve as an intermediary layer between the user's input and the LLM, analyzing and processing inputs to detect and neutralize potential prompt injections. By employing advanced natural language processing (NLP) techniques and pattern recognition, Javelin Processors can identify suspicious patterns, malicious commands, or anomalies indicative of an injection attempt."),(0,r.yg)("h2",{id:"integrating-with-prompt-injection-services"},"Integrating with Prompt Injection services"),(0,r.yg)("p",null,"You can also integrate off the shelf prompt-injection services like ",(0,r.yg)("a",{parentName:"p",href:"https://www.lakera.ai"},"Lakera"),", which provides a comprehensive suite of tools for detecting and preventing prompt injections. Lakera's services include real-time analysis, pattern matching, and anomaly detection, enabling you to safeguard your applications from prompt injection attacks."))}d.isMDXComponent=!0}}]);