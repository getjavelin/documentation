"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[496],{5680:(e,a,t)=>{t.d(a,{xA:()=>u,yg:()=>g});var n=t(6540);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function i(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function o(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?i(Object(t),!0).forEach((function(a){r(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function l(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var c=n.createContext({}),s=function(e){var a=n.useContext(c),t=a;return e&&(t="function"==typeof e?e(a):o(o({},a),e)),t},u=function(e){var a=s(e.components);return n.createElement(c.Provider,{value:a},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},f=n.forwardRef((function(e,a){var t=e.components,r=e.mdxType,i=e.originalType,c=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),p=s(t),f=r,g=p["".concat(c,".").concat(f)]||p[f]||d[f]||i;return t?n.createElement(g,o(o({ref:a},u),{},{components:t})):n.createElement(g,o({ref:a},u))}));function g(e,a){var t=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var i=t.length,o=new Array(i);o[0]=f;var l={};for(var c in a)hasOwnProperty.call(a,c)&&(l[c]=a[c]);l.originalType=e,l[p]="string"==typeof e?e:r,o[1]=l;for(var s=2;s<i;s++)o[s]=t[s];return n.createElement.apply(null,o)}return n.createElement.apply(null,t)}f.displayName="MDXCreateElement"},8459:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>c,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>l,toc:()=>s});var n=t(8168),r=(t(6540),t(5680));const i={},o="Load Balancing",l={unversionedId:"javelin-core/features/loadbalancing",id:"javelin-core/features/loadbalancing",title:"Load Balancing",description:"Organizations have several use cases that drive their load balancing needs.",source:"@site/docs/javelin-core/features/loadbalancing.md",sourceDirName:"javelin-core/features",slug:"/javelin-core/features/loadbalancing",permalink:"/docs/javelin-core/features/loadbalancing",draft:!1,editUrl:"https://github.com/getjavelin/documentation/tree/main/docs/javelin-core/features/loadbalancing.md",tags:[],version:"current",frontMatter:{},sidebar:"someSidebar",previous:{title:"Rate Limits",permalink:"/docs/javelin-core/features/ratelimits"},next:{title:"Semantic Caching",permalink:"/docs/javelin-core/features/caching"}},c={},s=[{value:"Use Cases",id:"use-cases",level:2},{value:"Traffic Load Shaping",id:"traffic-load-shaping",level:3},{value:"LLM Fallback",id:"llm-fallback",level:3},{value:"Credential Harvesting",id:"credential-harvesting",level:3}],u={toc:s},p="wrapper";function d(e){let{components:a,...t}=e;return(0,r.yg)(p,(0,n.A)({},u,t,{components:a,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"load-balancing"},"Load Balancing"),(0,r.yg)("p",null,"Organizations have several use cases that drive their load balancing needs. "),(0,r.yg)("h2",{id:"use-cases"},"Use Cases"),(0,r.yg)("h3",{id:"traffic-load-shaping"},"Traffic Load Shaping"),(0,r.yg)("p",null,"Organizations may want to strike a balance between inference accuracy, cost and latency by setting up a load shaping mix that shapes traffic towards multiple LLMs. "),(0,r.yg)("p",null,"For instance, an example traffic shape could be setup for using OpenAI GPT3.5 Turbo for 70% of the traffic and OpenAI GPT4 for 30% of the traffic. "),(0,r.yg)("h3",{id:"llm-fallback"},"LLM Fallback"),(0,r.yg)("p",null,"One way to manage inference costs is to setup LLM fallbacks when specific routes have exhausted their budgets. "),(0,r.yg)("p",null,"For instance, the route may be defined to use OpenAI GPT3.5 Turbo with a Google BARD fallback when the cost or policy budgets are exceeded. "),(0,r.yg)("h3",{id:"credential-harvesting"},"Credential Harvesting"),(0,r.yg)("p",null,"Many model providers enforce rate limiting on each of their credentials provisioned. As Application inference needs continually expand, they find that it may be beneficial to spread their load evenly across multiple credentials (or keys). "),(0,r.yg)("p",null,"For instance, the Application may choose to spread an anticipated load of 100 queries/second across 10 credential keys each with 10 queries/sec towards a specific model. "),(0,r.yg)("p",null,"Please contact: ",(0,r.yg)("a",{parentName:"p",href:"mailto:support@getjavelin.io"},"support@getjavelin.io")," if you would like to use this feature."))}d.isMDXComponent=!0}}]);