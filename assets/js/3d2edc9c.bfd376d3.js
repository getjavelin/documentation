"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[6574],{40401:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>o,default:()=>d,frontMatter:()=>l,metadata:()=>c,toc:()=>u});var a=t(74848),r=t(15680),s=t(11470),i=t(19365);const l={},o="Applications",c={id:"javelin-core/integration",title:"Applications",description:"Its easy to integrate applications that leverage LLMs with Javelin. We have made it easy to seamlessly connect your applications to route all LLM traffic through Javelin with 3 lines of code change.",source:"@site/docs/javelin-core/integration.md",sourceDirName:"javelin-core",slug:"/javelin-core/integration",permalink:"/docs/javelin-core/integration",draft:!1,unlisted:!1,editUrl:"https://github.com/getjavelin/documentation/tree/main/docs/javelin-core/integration.md",tags:[],version:"current",frontMatter:{},sidebar:"someSidebar",previous:{title:"Supported Large Language Models(LLMs)",permalink:"/docs/javelin-core/supported-llms"},next:{title:"Quickstart",permalink:"/docs/javelin-langchain-python/quickstart"}},p={},u=[{value:"Leveraging the Javelin Platform",id:"leveraging-the-javelin-platform",level:2},{value:"Querying an LLM",id:"querying-an-llm",level:2},{value:"REST API",id:"rest-api",level:3},{value:"Python",id:"python",level:3},{value:"JavaScript/TypeScript",id:"javascripttypescript",level:3}];function h(e){const n={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.RP)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"applications",children:"Applications"}),"\n",(0,a.jsx)(n.p,{children:"Its easy to integrate applications that leverage LLMs with Javelin. We have made it easy to seamlessly connect your applications to route all LLM traffic through Javelin with 3 lines of code change."}),"\n",(0,a.jsx)(n.h2,{id:"leveraging-the-javelin-platform",children:"Leveraging the Javelin Platform"}),"\n",(0,a.jsxs)(n.p,{children:["Rather than having your LLM Applications (like Co-Pilot apps etc.,) individually & directly point to the LLM Vendor & Model (like OpenAI, Gemini etc.,), configure the provider/model endpoint to be your Javelin endpoint. This ensures that all applications that leverage AI Models will route their requests through the gateway. Javelin supports all the ",(0,a.jsx)(n.a,{href:"supported-llms",children:"latest models and providers"}),", so you don't have to make any changes to your application or how requests to models are sent."]}),"\n",(0,a.jsxs)(n.p,{children:["See ",(0,a.jsx)(n.a,{href:"../javelin-python/quickstart",children:"Python SDK"})," for details on how you can easily embed this within your AI Apps."]}),"\n",(0,a.jsxs)(n.p,{children:["See ",(0,a.jsx)(n.a,{href:"routeconfiguration",children:"Javelin Configuration"})," section, for details on how to setup routes on the gateway to different models and providers."]}),"\n",(0,a.jsx)(n.h2,{id:"querying-an-llm",children:"Querying an LLM"}),"\n",(0,a.jsx)(n.p,{children:"Javelin may send a request to one or more models based on the configured policies and route configurations and return back a response."}),"\n",(0,a.jsx)(n.h3,{id:"rest-api",children:"REST API"}),"\n",(0,a.jsx)(s.A,{children:(0,a.jsx)(i.A,{value:"shell",label:"curl",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:'curl -X POST \\\n-H "Content-Type: application/json" \\\n-H "x-api-key: $JAVELIN_API_KEY" \\\n-H "Authorization: Bearer $OPENAI_API_KEY" \\\n-d \'{\n  "messages": [\n    {\n      "role": "system",\n      "content": "Hello, you are a helpful scientific assistant."\n    },\n    {\n      "role": "user",\n      "content": "What is the chemical composition of sugar?"\n    }\n  ],\n  "temperature": 0.8\n}\' \\\n"https://api-dev.javelin.live/v1/query/{routename}"\n'})})})}),"\n",(0,a.jsx)(n.h3,{id:"python",children:"Python"}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsxs)(i.A,{value:"py1",label:"Javelin SDK",children:[(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"pip install javelin-sdk\n"})}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-py",children:'from javelin_sdk import (\n    JavelinClient,\n    Route\n)\n\nimport os\n\njavelin_api_key = os.getenv(\'JAVELIN_API_KEY\')\nllm_api_key = os.getenv("OPENAI_API_KEY")\n\n# create javelin client\nclient = JavelinClient(javelin_api_key=javelin_api_key, \n                       llm_api_key=llm_api_key)\n\n# route name to get is {routename} e.g., sampleroute1\nquery_data = {\n    "messages": [ \n        {\n            "role": "system",\n            "content": "Hello, you are a helpful scientific assistant."\n        },\n        {\n            "role": "user",\n            "content": "What is the chemical composition of sugar?"\n        }\n    ],\n    "temperature": 0.8,\n}\n\n# now query the route, for async use `await client.aquery_route("sampleroute1", query_data)`\nresponse = client.query_route("sampleroute1", query_data)\nprint(response.model_dump_json(indent=2))\n\n'})})]}),(0,a.jsxs)(i.A,{value:"py2",label:"OpenAI",children:[(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"pip install openai\n"})}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-py",children:'from openai import OpenAI\nimport os\n\njavelin_api_key = os.environ[\'JAVELIN_API_KEY\']\nllm_api_key = os.environ["OPENAI_API_KEY"]\n\n# Javelin Headers\njavelin_headers = {\n                    "x-api-key": javelin_api_key,       # Javelin API key from admin\n                     "x-javelin-route": "sampleroute1"  # Javelin route to use\n                  }\n\n# Create OpenAI Client\nclient = OpenAI(api_key=llm_api_key,\n                base_url="https://api-dev.javelin.live/v1/query",\n                default_headers=javelin_headers)\n\n# Query the model\ncompletion = client.chat.completions.create(\n  model="gpt-3.5-turbo",\n  messages=[\n    {"role": "system", "content": "Hello, you are a helpful scientific assistant"},\n    {"role": "user", "content": "What is the chemical composition of sugar?"}\n  ])\n\nprint(completion.model_dump_json(indent=2))\n\n# Streaming Responses\nstream = client.chat.completions.create(\n    model="gpt-3.5-turbo",\n    messages=[\n      {"role": "system", "content": "Hello, you are a helpful scientific assistant."},\n      {"role": "user", "content": "What is the chemical composition of sugar?"}\n    ],\n    stream=True,\n)\n\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or "", end="")\n\n'})})]}),(0,a.jsxs)(i.A,{value:"py3",label:"Azure OpenAI",children:[(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell\x3c!--",children:"pip install openai\n"})}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-py",children:'from openai import AzureOpenAI\nimport os\n\n# Javelin Headers\njavelin_api_key = os.environ[\'JAVELIN_API_KEY\']\nllm_api_key = os.environ["AZURE_OPENAI_API_KEY"]\n\njavelin_headers = {\n                    "x-api-key": javelin_api_key,     # Javelin API key from admin\n                    "x-javelin-route": "sampleroute1" # Javelin route to use\n                  }\n\nclient = AzureOpenAI(api_key=llm_api_key,\n                     base_url="https://api-dev.javelin.live/v1/query",\n                     default_headers=javelin_headers,\n                     api_version="2023-07-01-preview")\n\ncompletion = client.chat.completions.create(\n  model="gpt-3.5-turbo",\n  messages=[\n    {"role": "system", "content": "Hello, you are a helpful scientific assistant."},\n    {"role": "user", "content": "What is the chemical composition of sugar?"}\n  ]\n)\n\nprint(completion.model_dump_json(indent=2))\n\n# Streaming Responses\nstream = client.chat.completions.create(\n    model="gpt-3.5-turbo",\n    messages=[\n      {"role": "system", "content": "Hello, you are a helpful scientific assistant."},\n      {"role": "user", "content": "What is the chemical composition of sugar?"}\n    ],\n    stream=True,\n)\n\nfor chunk in stream:\n  if chunk.choices:\n    print(chunk.choices[0].delta.content or "", end="")\n\n'})})]}),(0,a.jsxs)(i.A,{value:"py4",label:"LangChain",children:[(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell\x3c!--",children:"pip install langchain\npip install langchain-openai\n"})}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-py",children:'from langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nimport os\n\njavelin_api_key = os.getenv(\'JAVELIN_API_KEY\')\nllm_api_key = os.getenv("OPENAI_API_KEY")\njavelin_headers = {\n                    "x-api-key": javelin_api_key,      # Javelin API key from admin\n                    "x-javelin-route": "sample_route1" # Javelin route to use\n                  }\n\nllm = ChatOpenAI(\n    openai_api_base="https://api-dev.javelin.live/v1/query",\n    openai_api_key=llm_api_key,\n    model_kwargs={\n      "extra_headers": javelin_headers\n    },\n)\n\nprompt = ChatPromptTemplate.from_messages([\n    ("system", "Hello, you are a helpful scientific assistant."),\n    ("user", "{input}")\n])\n\noutput_parser = StrOutputParser()\n\nchain = prompt | llm | output_parser\n\nprint(chain.invoke({"input": "What is the chemical composition of sugar?"}))\n'})})]}),(0,a.jsxs)(i.A,{value:"py5",label:"DSPy",children:[(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Introduction:"})," ",(0,a.jsx)(n.a,{href:"https://towardsdatascience.com/intro-to-dspy-goodbye-prompting-hello-programming-4ca1c6ce3eb9",children:"DSPy: Goodbye Prompting, Hello Programming!"}),(0,a.jsx)(n.br,{}),"\n",(0,a.jsx)(n.strong,{children:"Documentation:"})," ",(0,a.jsx)(n.a,{href:"https://dspy-docs.vercel.app/",children:"DSPy Docs"})]}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell\x3c!--",children:"pip install dspy-ai\n"})}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-py",children:'import dspy\nfrom dsp import LM\nimport os\nimport requests\n\n# Assuming the environment variables are set correctly\njavelin_api_key = os.getenv(\'JAVELIN_API_KEY\')\nllm_api_key = os.getenv("OPENAI_API_KEY")\n\nclass Javelin(LM):\n    def __init__(self, model, api_key):\n        self.model = model\n        self.api_key = api_key\n        self.provider = "default"\n        self.kwargs = { \n                    "temperature": 1.0, \n                    "max_tokens": 500, \n                    "top_p": 1.0, \n                    "frequency_penalty": 0.0, \n                    "presence_penalty": 0.0, \n                    "stop": None, \n                    "n": 1, \n                    "logprobs": None, \n                    "logit_bias": None,\n                    "stream": False\n        }\n\n        self.base_url = "https://api-dev.javelin.live/v1/query/"\n        self.javelin_headers = {\n                    "Content-Type": "application/json",\n                    "Authorization": f"Bearer { api_key }",\n                    "x-javelin-route": "openai", # route name configured for OpenAI\n                    "x-api-key": javelin_api_key,\n        }\n\n        self.history = []\n\n    def basic_request(self, prompt: str, **kwargs):\n        headers = self.javelin_headers\n\n        data = {\n            **kwargs,\n            "model": self.model,\n            "messages": [\n                {"role": "user", "content": prompt}\n            ]\n        }\n\n        response = requests.post(self.base_url, headers=headers, json=data)\n        response = response.json()\n\n        self.history.append({\n            "prompt": prompt,\n            "response": response,\n            "kwargs": kwargs,\n        })\n        return response\n\n    def __call__(self, prompt, only_completed=True, return_sorted=False, **kwargs):\n        response = self.request(prompt, **kwargs)\n        if \'choices\' in response and len(response[\'choices\']) > 0:\n            first_choice_content = response[\'choices\'][0][\'message\'][\'content\']\n            completions = [first_choice_content]\n            return completions\n        else:\n            return ["No response found."]\n\njavelin = Javelin(model="gpt-4-1106-preview", api_key=llm_api_key)\ndspy.configure(lm=javelin)\n\n# Define a module (ChainOfThought) and assign it a signature (return an answer, given a question).\nqa = dspy.ChainOfThought(\'question -> answer\')\nresponse = qa(question="You have 3 baskets. The first basket has twice as many apples as the second basket. The third basket has 3 fewer apples than the first basket. If you have a total of 27 apples, how many apples are in each basket?")\nprint(response)\n'})})]}),(0,a.jsx)(i.A,{value:"py6",label:"...",children:(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://www.llamaindex.ai/open-source",children:"LlamaIndex"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://docs.datastax.com/en/ragstack/docs/index.html",children:"DataStax RAGStack"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://jxnl.github.io/instructor/",children:"Instructor, Generating Structure from LLMs"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://microsoft.github.io/promptflow/index.html#",children:"Microsoft Prompt flow"})}),"\n"]}),"\n"]})})]}),"\n",(0,a.jsx)(n.h3,{id:"javascripttypescript",children:"JavaScript/TypeScript"}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsxs)(i.A,{value:"js1",label:"OpenAI",children:[(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"npm install openai\n"})}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-js",children:'import OpenAI from "openai";\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n  baseURL: "https://api-dev.javelin.live/v1/query",\n  defaultHeaders: {\n    "x-api-key": `${process.env.JAVELIN_API_KEY}`,\n    "x-javelin-route": "sample_route1",\n  },\n});\n\nasync function main() {\n  const completion = await openai.chat.completions.create({\n    messages: [{ role: "system", content: "You are a helpful assistant." }],\n    model: "gpt-3.5-turbo",\n  });\n\n  console.log(completion.choices[0]);\n}\n\nmain();\n\n'})})]}),(0,a.jsxs)(i.A,{value:"js2",label:"Langchain",children:[(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"npm install @langchain/openai\n"})}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-js",children:'\nimport { ChatOpenAI } from \'@langchain/openai\';\n\nconst llm = new ChatOpenAI({\n    openAIApiKey: process.env.OPENAI_API_KEY,\n    configuration: {\n        basePath: "https://api-dev.javelin.live/v1/query",\n        defaultHeaders: {\n          "x-api-key": `${process.env.JAVELIN_API_KEY}`,\n          "x-javelin-route": "sample_route1",\n        },\n    },\n});\n\nasync function main() {\n  const response = await llm.invoke("tell me a joke?");\n  console.log(response);\n}\n\nmain();\n\n'})})]}),(0,a.jsxs)(i.A,{value:"js3",label:"...",children:[(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://sdk.vercel.ai/docs",children:"Vercel AI SDK"})," -> ",(0,a.jsx)(n.a,{href:"https://vercel.com/blog/ai-integrations",children:"AI Integrations on Vercel"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://github.com/huggingface/chat-ui",children:"Hugging Face ChatUI"})," -> ",(0,a.jsx)(n.a,{href:"https://github.com/huggingface/chat-ui?tab=readme-ov-file#running-your-own-models-using-a-custom-endpoint",children:"Running using a custom endpoint"})]}),"\n"]}),"\n"]}),(0,a.jsxs)(n.p,{children:["We have worked on the integrations. Please contact: ",(0,a.jsx)(n.a,{href:"mailto:support@getjavelin.io",children:"support@getjavelin.io"})," if you would like to use this feature."]})]})]})]})}function d(e={}){const{wrapper:n}={...(0,r.RP)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},15680:(e,n,t)=>{t.d(n,{RP:()=>c});var a=t(96540);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function s(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?s(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):s(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},s=Object.keys(e);for(a=0;a<s.length;a++)t=s[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(a=0;a<s.length;a++)t=s[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var o=a.createContext({}),c=function(e){var n=a.useContext(o),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},p={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},u=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,s=e.originalType,o=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),h=c(t),d=r,m=h["".concat(o,".").concat(d)]||h[d]||p[d]||s;return t?a.createElement(m,i(i({ref:n},u),{},{components:t})):a.createElement(m,i({ref:n},u))}));u.displayName="MDXCreateElement"},19365:(e,n,t)=>{t.d(n,{A:()=>i});t(96540);var a=t(18215);const r={tabItem:"tabItem_Ymn6"};var s=t(74848);function i(e){let{children:n,hidden:t,className:i}=e;return(0,s.jsx)("div",{role:"tabpanel",className:(0,a.A)(r.tabItem,i),hidden:t,children:n})}},11470:(e,n,t)=>{t.d(n,{A:()=>x});var a=t(96540),r=t(18215),s=t(23104),i=t(56347),l=t(205),o=t(57485),c=t(31682),p=t(70679);function u(e){return a.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function h(e){const{values:n,children:t}=e;return(0,a.useMemo)((()=>{const e=n??function(e){return u(e).map((e=>{let{props:{value:n,label:t,attributes:a,default:r}}=e;return{value:n,label:t,attributes:a,default:r}}))}(t);return function(e){const n=(0,c.X)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function d(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function m(e){let{queryString:n=!1,groupId:t}=e;const r=(0,i.W6)(),s=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,o.aZ)(s),(0,a.useCallback)((e=>{if(!s)return;const n=new URLSearchParams(r.location.search);n.set(s,e),r.replace({...r.location,search:n.toString()})}),[s,r])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:r}=e,s=h(e),[i,o]=(0,a.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!d({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const a=t.find((e=>e.default))??t[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:n,tabValues:s}))),[c,u]=m({queryString:t,groupId:r}),[f,v]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[r,s]=(0,p.Dv)(t);return[r,(0,a.useCallback)((e=>{t&&s.set(e)}),[t,s])]}({groupId:r}),g=(()=>{const e=c??f;return d({value:e,tabValues:s})?e:null})();(0,l.A)((()=>{g&&o(g)}),[g]);return{selectedValue:i,selectValue:(0,a.useCallback)((e=>{if(!d({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);o(e),u(e),v(e)}),[u,v,s]),tabValues:s}}var v=t(92303);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var y=t(74848);function j(e){let{className:n,block:t,selectedValue:a,selectValue:i,tabValues:l}=e;const o=[],{blockElementScrollPositionUntilNextRender:c}=(0,s.a_)(),p=e=>{const n=e.currentTarget,t=o.indexOf(n),r=l[t].value;r!==a&&(c(n),i(r))},u=e=>{let n=null;switch(e.key){case"Enter":p(e);break;case"ArrowRight":{const t=o.indexOf(e.currentTarget)+1;n=o[t]??o[0];break}case"ArrowLeft":{const t=o.indexOf(e.currentTarget)-1;n=o[t]??o[o.length-1];break}}n?.focus()};return(0,y.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":t},n),children:l.map((e=>{let{value:n,label:t,attributes:s}=e;return(0,y.jsx)("li",{role:"tab",tabIndex:a===n?0:-1,"aria-selected":a===n,ref:e=>o.push(e),onKeyDown:u,onClick:p,...s,className:(0,r.A)("tabs__item",g.tabItem,s?.className,{"tabs__item--active":a===n}),children:t??n},n)}))})}function _(e){let{lazy:n,children:t,selectedValue:r}=e;const s=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=s.find((e=>e.props.value===r));return e?(0,a.cloneElement)(e,{className:"margin-top--md"}):null}return(0,y.jsx)("div",{className:"margin-top--md",children:s.map(((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==r})))})}function b(e){const n=f(e);return(0,y.jsxs)("div",{className:(0,r.A)("tabs-container",g.tabList),children:[(0,y.jsx)(j,{...n,...e}),(0,y.jsx)(_,{...n,...e})]})}function x(e){const n=(0,v.A)();return(0,y.jsx)(b,{...e,children:u(e.children)},String(n))}}}]);