"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[421],{48453:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>a,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"javelin-processors/contentmoderation","title":"Taxonomies","description":"In the era of digital communication and user-generated content, content moderation has become crucial for maintaining the integrity, safety, and user-friendliness of online platforms. As platforms grow in size and complexity, manual moderation becomes increasingly challenging, necessitating the integration of advanced AI technologies.","source":"@site/docs/javelin-processors/contentmoderation.md","sourceDirName":"javelin-processors","slug":"/javelin-processors/contentmoderation","permalink":"/docs/javelin-processors/contentmoderation","draft":false,"unlisted":false,"editUrl":"https://github.com/getjavelin/documentation/tree/main/docs/javelin-processors/contentmoderation.md","tags":[],"version":"current","frontMatter":{},"sidebar":"someSidebar","previous":{"title":"Overview","permalink":"/docs/javelin-processors/trustsafety"},"next":{"title":"Usage","permalink":"/docs/javelin-processors/trustsafetyusage"}}');var s=t(74848),i=t(28453);const a={},r="Taxonomies",c={},l=[{value:"How it works",id:"how-it-works",level:2},{value:"Supported Taxonomies",id:"supported-taxonomies",level:2},{value:"Integrating with Content Moderation Services",id:"integrating-with-content-moderation-services",level:2}];function d(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"taxonomies",children:"Taxonomies"})}),"\n",(0,s.jsx)(n.p,{children:"In the era of digital communication and user-generated content, content moderation has become crucial for maintaining the integrity, safety, and user-friendliness of online platforms. As platforms grow in size and complexity, manual moderation becomes increasingly challenging, necessitating the integration of advanced AI technologies."}),"\n",(0,s.jsx)(n.p,{children:"Javelin Processors, integrated with AI capabilities, present a sophisticated solution for automating content moderation tasks, ensuring that online environments remain safe, respectful, and compliant with platform policies and societal norms."}),"\n",(0,s.jsx)(n.p,{children:"Javelin providers a safety risk taxonomy associated with interacting with AI agents. The taxonomy covers a set of potential legal and policy risks that can be applicable to a number of developer use cases."}),"\n",(0,s.jsx)(n.h2,{id:"how-it-works",children:"How it works"}),"\n",(0,s.jsx)(n.p,{children:"Javelin uses an internal LLM-based input-output safeguard model, fine-tuned on data labeled according to this taxonomy. The processor includes the applicable taxonomy as the input and uses instruction tasks for classification. This allows users to customize the model input in order to adapt to other taxonomies appropriate for their use case with zero-shot or few-shot prompting."}),"\n",(0,s.jsx)(n.h2,{id:"supported-taxonomies",children:"Supported Taxonomies"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Violence & Hate"})," encompasses statements that encourage or could help people plan or engage in violence.\nSimilarly, statements that advocate discrimination, contain slurs, or voice hateful sentiments against\npeople based on their sensitive personal characteristics (ex: race, color, religion, national origin, sexual\norientation, gender, gender identity, or disability) would also be considered inappropriate under this\ncategory."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Sexual Content"})," encompasses statements encouraging someone (who could be underage) to engage\nin specific sex acts. Similarly, sexually explicit (i.e., erotic) statements would also be considered\ninappropriate under this category."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Guns & Illegal Weapons"})," encompasses statements that specifically encourage, condone, or could help\npeople plan or execute specific crimes involving the illegal acquisition, creation, or use of guns or illegal\nweapons (ex: explosives, biological agents, or chemical weapons), including instructions on how to create\nor use them."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Regulated or Controlled Substances"})," encompasses statements that specifically encourage or could help\npeople to illegally produce, transfer, or consume regulated or controlled substances like illegal drugs,\ntobacco, alcohol, or cannabis."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Suicide & Self Harm"})," encompasses statements that encourage, condone, or enable people to harm\nthemselves (ex: by providing instructions or information on methods of self-harm). When people express\nor imply an intent to harm themselves, any response other than one that provides links to appropriate\nhealth resources would be considered inappropriate (even if entirely supportive)."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Criminal Planning"})," encompasses miscellaneous statements that encourage, condone, or could help people\nplan or execute specific criminal activities, like arson, kidnapping, or theft. Items should only be\nconsidered harmful under this category when they could not be read as violating any of the other harm\ntypes above (ex: statements that encourage violence should be considered violating under Violence &\nHate rather than this category)."]}),"\n",(0,s.jsx)(n.h2,{id:"integrating-with-content-moderation-services",children:"Integrating with Content Moderation Services"}),"\n",(0,s.jsx)(n.p,{children:"Javelin processors may also be used to easily intergate with external content moderation endpoints from providers such as:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/moderation",children:"OpenAI Content Moderation"}),","]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://azure.microsoft.com/en-us/products/ai-services/ai-content-safety/",children:"Azure AI Content Safety"})," or"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://www.lakera.ai",children:"Lakera Content Moderation"})," etc.,"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"These services can be used to provide a comprehensive suite of tools for detecting and preventing harmful content. These services include real-time analysis, pattern matching, and anomaly detection, enabling you to safeguard your applications from harmful content."})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var o=t(96540);const s={},i=o.createContext(s);function a(e){const n=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);