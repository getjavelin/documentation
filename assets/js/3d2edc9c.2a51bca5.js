"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[6574],{5680:(e,n,t)=>{t.d(n,{xA:()=>p,yg:()=>d});var a=t(6540);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function l(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?l(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):l(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function i(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},l=Object.keys(e);for(a=0;a<l.length;a++)t=l[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(a=0;a<l.length;a++)t=l[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var s=a.createContext({}),u=function(e){var n=a.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},p=function(e){var n=u(e.components);return a.createElement(s.Provider,{value:n},e.children)},c="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},y=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,l=e.originalType,s=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),c=u(t),y=r,d=c["".concat(s,".").concat(y)]||c[y]||m[y]||l;return t?a.createElement(d,o(o({ref:n},p),{},{components:t})):a.createElement(d,o({ref:n},p))}));function d(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var l=t.length,o=new Array(l);o[0]=y;var i={};for(var s in n)hasOwnProperty.call(n,s)&&(i[s]=n[s]);i.originalType=e,i[c]="string"==typeof e?e:r,o[1]=i;for(var u=2;u<l;u++)o[u]=t[u];return a.createElement.apply(null,o)}return a.createElement.apply(null,t)}y.displayName="MDXCreateElement"},9365:(e,n,t)=>{t.d(n,{A:()=>o});var a=t(6540),r=t(53);const l={tabItem:"tabItem_Ymn6"};function o(e){let{children:n,hidden:t,className:o}=e;return a.createElement("div",{role:"tabpanel",className:(0,r.A)(l.tabItem,o),hidden:t},n)}},1470:(e,n,t)=>{t.d(n,{A:()=>A});var a=t(8168),r=t(6540),l=t(53),o=t(3104),i=t(6347),s=t(7485),u=t(1682),p=t(9466);function c(e){return function(e){return r.Children.map(e,(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:n,label:t,attributes:a,default:r}}=e;return{value:n,label:t,attributes:a,default:r}}))}function m(e){const{values:n,children:t}=e;return(0,r.useMemo)((()=>{const e=n??c(t);return function(e){const n=(0,u.X)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function y(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function d(e){let{queryString:n=!1,groupId:t}=e;const a=(0,i.W6)(),l=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,s.aZ)(l),(0,r.useCallback)((e=>{if(!l)return;const n=new URLSearchParams(a.location.search);n.set(l,e),a.replace({...a.location,search:n.toString()})}),[l,a])]}function g(e){const{defaultValue:n,queryString:t=!1,groupId:a}=e,l=m(e),[o,i]=(0,r.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!y({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const a=t.find((e=>e.default))??t[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:n,tabValues:l}))),[s,u]=d({queryString:t,groupId:a}),[c,g]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[a,l]=(0,p.Dv)(t);return[a,(0,r.useCallback)((e=>{t&&l.set(e)}),[t,l])]}({groupId:a}),h=(()=>{const e=s??c;return y({value:e,tabValues:l})?e:null})();(0,r.useLayoutEffect)((()=>{h&&i(h)}),[h]);return{selectedValue:o,selectValue:(0,r.useCallback)((e=>{if(!y({value:e,tabValues:l}))throw new Error(`Can't select invalid tab value=${e}`);i(e),u(e),g(e)}),[u,g,l]),tabValues:l}}var h=t(2303);const v={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function f(e){let{className:n,block:t,selectedValue:i,selectValue:s,tabValues:u}=e;const p=[],{blockElementScrollPositionUntilNextRender:c}=(0,o.a_)(),m=e=>{const n=e.currentTarget,t=p.indexOf(n),a=u[t].value;a!==i&&(c(n),s(a))},y=e=>{let n=null;switch(e.key){case"Enter":m(e);break;case"ArrowRight":{const t=p.indexOf(e.currentTarget)+1;n=p[t]??p[0];break}case"ArrowLeft":{const t=p.indexOf(e.currentTarget)-1;n=p[t]??p[p.length-1];break}}n?.focus()};return r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,l.A)("tabs",{"tabs--block":t},n)},u.map((e=>{let{value:n,label:t,attributes:o}=e;return r.createElement("li",(0,a.A)({role:"tab",tabIndex:i===n?0:-1,"aria-selected":i===n,key:n,ref:e=>p.push(e),onKeyDown:y,onClick:m},o,{className:(0,l.A)("tabs__item",v.tabItem,o?.className,{"tabs__item--active":i===n})}),t??n)})))}function b(e){let{lazy:n,children:t,selectedValue:a}=e;const l=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=l.find((e=>e.props.value===a));return e?(0,r.cloneElement)(e,{className:"margin-top--md"}):null}return r.createElement("div",{className:"margin-top--md"},l.map(((e,n)=>(0,r.cloneElement)(e,{key:n,hidden:e.props.value!==a}))))}function _(e){const n=g(e);return r.createElement("div",{className:(0,l.A)("tabs-container",v.tabList)},r.createElement(f,(0,a.A)({},e,n)),r.createElement(b,(0,a.A)({},e,n)))}function A(e){const n=(0,h.A)();return r.createElement(_,(0,a.A)({key:String(n)},e))}},5208:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>s,default:()=>d,frontMatter:()=>i,metadata:()=>u,toc:()=>c});var a=t(8168),r=(t(6540),t(5680)),l=t(1470),o=t(9365);const i={},s="Applications",u={unversionedId:"javelin-core/integration",id:"javelin-core/integration",title:"Applications",description:"Its easy to integrate applications that leverage LLMs with Javelin. We have made it easy to seamlessly connect your applications to route all LLM traffic through Javelin with 3 lines of code change.",source:"@site/docs/javelin-core/integration.md",sourceDirName:"javelin-core",slug:"/javelin-core/integration",permalink:"/docs/javelin-core/integration",draft:!1,editUrl:"https://github.com/getjavelin/documentation/tree/main/docs/javelin-core/integration.md",tags:[],version:"current",frontMatter:{},sidebar:"someSidebar",previous:{title:"Supported Large Language Models(LLMs)",permalink:"/docs/javelin-core/supported-llms"},next:{title:"Quickstart",permalink:"/docs/javelin-langchain-python/quickstart"}},p={},c=[{value:"Leveraging the Javelin Platform",id:"leveraging-the-javelin-platform",level:2},{value:"Querying an LLM",id:"querying-an-llm",level:2},{value:"REST API",id:"rest-api",level:3},{value:"Python",id:"python",level:3},{value:"JavaScript/TypeScript",id:"javascripttypescript",level:3}],m={toc:c},y="wrapper";function d(e){let{components:n,...t}=e;return(0,r.yg)(y,(0,a.A)({},m,t,{components:n,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"applications"},"Applications"),(0,r.yg)("p",null,"Its easy to integrate applications that leverage LLMs with Javelin. We have made it easy to seamlessly connect your applications to route all LLM traffic through Javelin with 3 lines of code change."),(0,r.yg)("h2",{id:"leveraging-the-javelin-platform"},"Leveraging the Javelin Platform"),(0,r.yg)("p",null,"Rather than having your LLM Applications (like Co-Pilot apps etc.,) individually & directly point to the LLM Vendor & Model (like OpenAI, Gemini etc.,), configure the provider/model endpoint to be your Javelin endpoint. This ensures that all applications that leverage AI Models will route their requests through the gateway. Javelin supports all the ",(0,r.yg)("a",{parentName:"p",href:"supported-llms"},"latest models and providers"),", so you don't have to make any changes to your application or how requests to models are sent. "),(0,r.yg)("p",null,"See ",(0,r.yg)("a",{parentName:"p",href:"../javelin-python/quickstart"},"Python SDK")," for details on how you can easily embed this within your AI Apps. "),(0,r.yg)("p",null,"See ",(0,r.yg)("a",{parentName:"p",href:"routeconfiguration"},"Javelin Configuration")," section, for details on how to setup routes on the gateway to different models and providers. "),(0,r.yg)("h2",{id:"querying-an-llm"},"Querying an LLM"),(0,r.yg)("p",null,"Javelin may send a request to one or more models based on the configured policies and route configurations and return back a response."),(0,r.yg)("h3",{id:"rest-api"},"REST API"),(0,r.yg)(l.A,{mdxType:"Tabs"},(0,r.yg)(o.A,{value:"shell",label:"curl",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell"},'curl -X POST \\\n-H "Content-Type: application/json" \\\n-H "x-api-key: $JAVELIN_API_KEY" \\\n-H "Authorization: Bearer $OPENAI_API_KEY" \\\n-d \'{\n  "messages": [\n    {\n      "role": "system",\n      "content": "Hello, you are a helpful scientific assistant."\n    },\n    {\n      "role": "user",\n      "content": "What is the chemical composition of sugar?"\n    }\n  ],\n  "temperature": 0.8\n}\' \\\n"https://api.javelin.live/v1/query/{routename}"\n')))),(0,r.yg)("h3",{id:"python"},"Python"),(0,r.yg)(l.A,{mdxType:"Tabs"},(0,r.yg)(o.A,{value:"py1",label:"Javelin SDK",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell"},"pip install javelin-sdk\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-py"},'from javelin_sdk import (\n    JavelinClient,\n    Route\n)\n\nimport os\n\njavelin_api_key = os.getenv(\'JAVELIN_API_KEY\')\nllm_api_key = os.getenv("OPENAI_API_KEY")\n\n# create javelin client\nclient = JavelinClient(javelin_api_key=javelin_api_key, \n                       llm_api_key=llm_api_key)\n\n# route name to get is {routename} e.g., sampleroute1\nquery_data = {\n    "messages": [ \n        {\n            "role": "system",\n            "content": "Hello, you are a helpful scientific assistant."\n        },\n        {\n            "role": "user",\n            "content": "What is the chemical composition of sugar?"\n        }\n    ],\n    "temperature": 0.8,\n}\n\n# now query the route, for async use `await client.aquery_route("sampleroute1", query_data)`\nresponse = client.query_route("sampleroute1", query_data)\nprint(response.model_dump_json(indent=2))\n\n'))),(0,r.yg)(o.A,{value:"py2",label:"OpenAI",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell"},"pip install openai\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-py"},'from openai import OpenAI\nimport os\n\njavelin_api_key = os.environ[\'JAVELIN_API_KEY\']\nllm_api_key = os.environ["OPENAI_API_KEY"]\n\n# Javelin Headers\njavelin_headers = {\n                    "x-api-key": javelin_api_key,       # Javelin API key from admin\n                     "x-javelin-route": "sampleroute1"  # Javelin route to use\n                  }\n\n# Create OpenAI Client\nclient = OpenAI(api_key=llm_api_key,\n                base_url="https://api.javelin.live/v1/query",\n                default_headers=javelin_headers)\n\n# Query the model\ncompletion = client.chat.completions.create(\n  model="gpt-3.5-turbo",\n  messages=[\n    {"role": "system", "content": "Hello, you are a helpful scientific assistant"},\n    {"role": "user", "content": "What is the chemical composition of sugar?"}\n  ])\n\nprint(completion.model_dump_json(indent=2))\n\n# Streaming Responses\nstream = client.chat.completions.create(\n    model="gpt-3.5-turbo",\n    messages=[\n      {"role": "system", "content": "Hello, you are a helpful scientific assistant."},\n      {"role": "user", "content": "What is the chemical composition of sugar?"}\n    ],\n    stream=True,\n)\n\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or "", end="")\n\n'))),(0,r.yg)(o.A,{value:"py3",label:"Azure OpenAI",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell\x3c!--"},"pip install openai\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-py"},'from openai import AzureOpenAI\nimport os\n\n# Javelin Headers\njavelin_api_key = os.environ[\'JAVELIN_API_KEY\']\nllm_api_key = os.environ["AZURE_OPENAI_API_KEY"]\n\njavelin_headers = {\n                    "x-api-key": javelin_api_key,     # Javelin API key from admin\n                    "x-javelin-route": "sampleroute1" # Javelin route to use\n                  }\n\nclient = AzureOpenAI(api_key=llm_api_key,\n                     base_url="https://api.javelin.live/v1/query",\n                     default_headers=javelin_headers,\n                     api_version="2023-07-01-preview")\n\ncompletion = client.chat.completions.create(\n  model="gpt-3.5-turbo",\n  messages=[\n    {"role": "system", "content": "Hello, you are a helpful scientific assistant."},\n    {"role": "user", "content": "What is the chemical composition of sugar?"}\n  ]\n)\n\nprint(completion.model_dump_json(indent=2))\n\n# Streaming Responses\nstream = client.chat.completions.create(\n    model="gpt-3.5-turbo",\n    messages=[\n      {"role": "system", "content": "Hello, you are a helpful scientific assistant."},\n      {"role": "user", "content": "What is the chemical composition of sugar?"}\n    ],\n    stream=True,\n)\n\nfor chunk in stream:\n  if chunk.choices:\n    print(chunk.choices[0].delta.content or "", end="")\n\n'))),(0,r.yg)(o.A,{value:"py4",label:"LangChain",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell\x3c!--"},"pip install langchain\npip install langchain-openai\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-py"},'from langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nimport os\n\njavelin_api_key = os.getenv(\'JAVELIN_API_KEY\')\nllm_api_key = os.getenv("OPENAI_API_KEY")\njavelin_headers = {\n                    "x-api-key": javelin_api_key,      # Javelin API key from admin\n                    "x-javelin-route": "sample_route1" # Javelin route to use\n                  }\n\nllm = ChatOpenAI(\n    openai_api_base="https://api.javelin.live/v1/query",\n    openai_api_key=llm_api_key,\n    model_kwargs={\n      "extra_headers": javelin_headers\n    },\n)\n\nprompt = ChatPromptTemplate.from_messages([\n    ("system", "Hello, you are a helpful scientific assistant."),\n    ("user", "{input}")\n])\n\noutput_parser = StrOutputParser()\n\nchain = prompt | llm | output_parser\n\nprint(chain.invoke({"input": "What is the chemical composition of sugar?"}))\n')))),(0,r.yg)("h3",{id:"javascripttypescript"},"JavaScript/TypeScript"),(0,r.yg)(l.A,{mdxType:"Tabs"},(0,r.yg)(o.A,{value:"js1",label:"OpenAI",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell"},"npm install openai\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-js"},'import OpenAI from "openai";\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n  baseURL: "https://api.javelin.live/v1/query",\n  defaultHeaders: {\n    "x-api-key": `${process.env.JAVELIN_API_KEY}`,\n    "x-javelin-route": "sample_route1",\n  },\n});\n\nasync function main() {\n  const completion = await openai.chat.completions.create({\n    messages: [{ role: "system", content: "You are a helpful assistant." }],\n    model: "gpt-3.5-turbo",\n  });\n\n  console.log(completion.choices[0]);\n}\n\nmain();\n\n'))),(0,r.yg)(o.A,{value:"js2",label:"Langchain",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell"},"npm install @langchain/openai\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-js"},'\nimport { ChatOpenAI } from \'@langchain/openai\';\n\nconst llm = new ChatOpenAI({\n    openAIApiKey: process.env.OPENAI_API_KEY,\n    configuration: {\n        basePath: "https://api.javelin.live/v1/query",\n        defaultHeaders: {\n          "x-api-key": `${process.env.JAVELIN_API_KEY}`,\n          "x-javelin-route": "sample_route1",\n        },\n    },\n});\n\nasync function main() {\n  const response = await llm.invoke("tell me a joke?");\n  console.log(response);\n}\n\nmain();\n\n')))))}d.isMDXComponent=!0}}]);