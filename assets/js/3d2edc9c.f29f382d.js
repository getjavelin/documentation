"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[6574],{5680:(e,n,a)=>{a.d(n,{xA:()=>u,yg:()=>g});var t=a(6540);function r(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function l(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,t)}return a}function o(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?l(Object(a),!0).forEach((function(n){r(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function i(e,n){if(null==e)return{};var a,t,r=function(e,n){if(null==e)return{};var a,t,r={},l=Object.keys(e);for(t=0;t<l.length;t++)a=l[t],n.indexOf(a)>=0||(r[a]=e[a]);return r}(e,n);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(t=0;t<l.length;t++)a=l[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=t.createContext({}),p=function(e){var n=t.useContext(s),a=n;return e&&(a="function"==typeof e?e(n):o(o({},n),e)),a},u=function(e){var n=p(e.components);return t.createElement(s.Provider,{value:n},e.children)},c="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},y=t.forwardRef((function(e,n){var a=e.components,r=e.mdxType,l=e.originalType,s=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),c=p(a),y=r,g=c["".concat(s,".").concat(y)]||c[y]||m[y]||l;return a?t.createElement(g,o(o({ref:n},u),{},{components:a})):t.createElement(g,o({ref:n},u))}));function g(e,n){var a=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var l=a.length,o=new Array(l);o[0]=y;var i={};for(var s in n)hasOwnProperty.call(n,s)&&(i[s]=n[s]);i.originalType=e,i[c]="string"==typeof e?e:r,o[1]=i;for(var p=2;p<l;p++)o[p]=a[p];return t.createElement.apply(null,o)}return t.createElement.apply(null,a)}y.displayName="MDXCreateElement"},9365:(e,n,a)=>{a.d(n,{A:()=>o});var t=a(6540),r=a(53);const l={tabItem:"tabItem_Ymn6"};function o(e){let{children:n,hidden:a,className:o}=e;return t.createElement("div",{role:"tabpanel",className:(0,r.A)(l.tabItem,o),hidden:a},n)}},1470:(e,n,a)=>{a.d(n,{A:()=>A});var t=a(8168),r=a(6540),l=a(53),o=a(3104),i=a(6347),s=a(7485),p=a(1682),u=a(9466);function c(e){return function(e){return r.Children.map(e,(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:n,label:a,attributes:t,default:r}}=e;return{value:n,label:a,attributes:t,default:r}}))}function m(e){const{values:n,children:a}=e;return(0,r.useMemo)((()=>{const e=n??c(a);return function(e){const n=(0,p.X)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,a])}function y(e){let{value:n,tabValues:a}=e;return a.some((e=>e.value===n))}function g(e){let{queryString:n=!1,groupId:a}=e;const t=(0,i.W6)(),l=function(e){let{queryString:n=!1,groupId:a}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:n,groupId:a});return[(0,s.aZ)(l),(0,r.useCallback)((e=>{if(!l)return;const n=new URLSearchParams(t.location.search);n.set(l,e),t.replace({...t.location,search:n.toString()})}),[l,t])]}function d(e){const{defaultValue:n,queryString:a=!1,groupId:t}=e,l=m(e),[o,i]=(0,r.useState)((()=>function(e){let{defaultValue:n,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!y({value:n,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const t=a.find((e=>e.default))??a[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:l}))),[s,p]=g({queryString:a,groupId:t}),[c,d]=function(e){let{groupId:n}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(n),[t,l]=(0,u.Dv)(a);return[t,(0,r.useCallback)((e=>{a&&l.set(e)}),[a,l])]}({groupId:t}),h=(()=>{const e=s??c;return y({value:e,tabValues:l})?e:null})();(0,r.useLayoutEffect)((()=>{h&&i(h)}),[h]);return{selectedValue:o,selectValue:(0,r.useCallback)((e=>{if(!y({value:e,tabValues:l}))throw new Error(`Can't select invalid tab value=${e}`);i(e),p(e),d(e)}),[p,d,l]),tabValues:l}}var h=a(2303);const v={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function f(e){let{className:n,block:a,selectedValue:i,selectValue:s,tabValues:p}=e;const u=[],{blockElementScrollPositionUntilNextRender:c}=(0,o.a_)(),m=e=>{const n=e.currentTarget,a=u.indexOf(n),t=p[a].value;t!==i&&(c(n),s(t))},y=e=>{let n=null;switch(e.key){case"Enter":m(e);break;case"ArrowRight":{const a=u.indexOf(e.currentTarget)+1;n=u[a]??u[0];break}case"ArrowLeft":{const a=u.indexOf(e.currentTarget)-1;n=u[a]??u[u.length-1];break}}n?.focus()};return r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,l.A)("tabs",{"tabs--block":a},n)},p.map((e=>{let{value:n,label:a,attributes:o}=e;return r.createElement("li",(0,t.A)({role:"tab",tabIndex:i===n?0:-1,"aria-selected":i===n,key:n,ref:e=>u.push(e),onKeyDown:y,onClick:m},o,{className:(0,l.A)("tabs__item",v.tabItem,o?.className,{"tabs__item--active":i===n})}),a??n)})))}function _(e){let{lazy:n,children:a,selectedValue:t}=e;const l=(Array.isArray(a)?a:[a]).filter(Boolean);if(n){const e=l.find((e=>e.props.value===t));return e?(0,r.cloneElement)(e,{className:"margin-top--md"}):null}return r.createElement("div",{className:"margin-top--md"},l.map(((e,n)=>(0,r.cloneElement)(e,{key:n,hidden:e.props.value!==t}))))}function b(e){const n=d(e);return r.createElement("div",{className:(0,l.A)("tabs-container",v.tabList)},r.createElement(f,(0,t.A)({},e,n)),r.createElement(_,(0,t.A)({},e,n)))}function A(e){const n=(0,h.A)();return r.createElement(b,(0,t.A)({key:String(n)},e))}},5208:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>u,contentTitle:()=>s,default:()=>g,frontMatter:()=>i,metadata:()=>p,toc:()=>c});var t=a(8168),r=(a(6540),a(5680)),l=a(1470),o=a(9365);const i={},s="Applications",p={unversionedId:"javelin-core/integration",id:"javelin-core/integration",title:"Applications",description:"Its easy to integrate applications that leverage LLMs with Javelin. We have made it easy to seamlessly connect your applications to route all LLM traffic through Javelin with 3 lines of code change.",source:"@site/docs/javelin-core/integration.md",sourceDirName:"javelin-core",slug:"/javelin-core/integration",permalink:"/docs/javelin-core/integration",draft:!1,editUrl:"https://github.com/getjavelin/documentation/tree/main/docs/javelin-core/integration.md",tags:[],version:"current",frontMatter:{},sidebar:"someSidebar",previous:{title:"Supported Large Language Models(LLMs)",permalink:"/docs/javelin-core/supported-llms"},next:{title:"Quickstart",permalink:"/docs/javelin-langchain-python/quickstart"}},u={},c=[{value:"Leveraging the Javelin Platform",id:"leveraging-the-javelin-platform",level:2},{value:"Querying an LLM",id:"querying-an-llm",level:2},{value:"REST API",id:"rest-api",level:3},{value:"Python",id:"python",level:3},{value:"JavaScript/TypeScript",id:"javascripttypescript",level:3}],m={toc:c},y="wrapper";function g(e){let{components:n,...a}=e;return(0,r.yg)(y,(0,t.A)({},m,a,{components:n,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"applications"},"Applications"),(0,r.yg)("p",null,"Its easy to integrate applications that leverage LLMs with Javelin. We have made it easy to seamlessly connect your applications to route all LLM traffic through Javelin with 3 lines of code change."),(0,r.yg)("h2",{id:"leveraging-the-javelin-platform"},"Leveraging the Javelin Platform"),(0,r.yg)("p",null,"Rather than having your LLM Applications (like Co-Pilot apps etc.,) individually & directly point to the LLM Vendor & Model (like OpenAI, Gemini etc.,), configure the provider/model endpoint to be your Javelin endpoint. This ensures that all applications that leverage AI Models will route their requests through the gateway. Javelin supports all the ",(0,r.yg)("a",{parentName:"p",href:"supported-llms"},"latest models and providers"),", so you don't have to make any changes to your application or how requests to models are sent. "),(0,r.yg)("p",null,"See ",(0,r.yg)("a",{parentName:"p",href:"../javelin-python/quickstart"},"Python SDK")," for details on how you can easily embed this within your AI Apps. "),(0,r.yg)("p",null,"See ",(0,r.yg)("a",{parentName:"p",href:"routeconfiguration"},"Javelin Configuration")," section, for details on how to setup routes on the gateway to different models and providers. "),(0,r.yg)("h2",{id:"querying-an-llm"},"Querying an LLM"),(0,r.yg)("p",null,"Javelin may send a request to one or more models based on the configured policies and route configurations and return back a response."),(0,r.yg)("h3",{id:"rest-api"},"REST API"),(0,r.yg)(l.A,{mdxType:"Tabs"},(0,r.yg)(o.A,{value:"shell",label:"curl",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell"},'curl -X POST \\\n-H "Content-Type: application/json" \\\n-H "x-api-key: $JAVELIN_API_KEY" \\\n-H "Authorization: Bearer $OPENAI_API_KEY" \\\n-d \'{\n  "messages": [\n    {\n      "role": "system",\n      "content": "Hello, you are a helpful scientific assistant."\n    },\n    {\n      "role": "user",\n      "content": "What is the chemical composition of sugar?"\n    }\n  ],\n  "temperature": 0.8\n}\' \\\n"https://api.javelin.live/v1/query/{routename}"\n')))),(0,r.yg)("h3",{id:"python"},"Python"),(0,r.yg)(l.A,{mdxType:"Tabs"},(0,r.yg)(o.A,{value:"py1",label:"Javelin SDK",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell"},"pip install javelin-sdk\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-py"},'from javelin_sdk import (\n    JavelinClient,\n    Route\n)\n\nimport os\n\njavelin_api_key = os.getenv(\'JAVELIN_API_KEY\')\nllm_api_key = os.getenv("OPENAI_API_KEY")\n\n# create javelin client\nclient = JavelinClient(javelin_api_key=javelin_api_key, \n                       llm_api_key=llm_api_key)\n\n# route name to get is {routename} e.g., sampleroute1\nquery_data = {\n    "messages": [ \n        {\n            "role": "system",\n            "content": "Hello, you are a helpful scientific assistant."\n        },\n        {\n            "role": "user",\n            "content": "What is the chemical composition of sugar?"\n        }\n    ],\n    "temperature": 0.8,\n}\n\n# now query the route, for async use `await client.aquery_route("sampleroute1", query_data)`\nresponse = client.query_route("sampleroute1", query_data)\nprint(response.model_dump_json(indent=2))\n\n'))),(0,r.yg)(o.A,{value:"py2",label:"OpenAI",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell"},"pip install openai\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-py"},'from openai import OpenAI\nimport os\n\njavelin_api_key = os.environ[\'JAVELIN_API_KEY\']\nllm_api_key = os.environ["OPENAI_API_KEY"]\n\n# Javelin Headers\njavelin_headers = {\n                    "x-api-key": javelin_api_key,       # Javelin API key from admin\n                     "x-javelin-route": "sampleroute1"  # Javelin route to use\n                  }\n\n# Create OpenAI Client\nclient = OpenAI(api_key=llm_api_key,\n                base_url="https://api.javelin.live/v1/query",\n                default_headers=javelin_headers)\n\n# Query the model\ncompletion = client.chat.completions.create(\n  model="gpt-3.5-turbo",\n  messages=[\n    {"role": "system", "content": "Hello, you are a helpful scientific assistant"},\n    {"role": "user", "content": "What is the chemical composition of sugar?"}\n  ])\n\nprint(completion.model_dump_json(indent=2))\n\n# Streaming Responses\nstream = client.chat.completions.create(\n    model="gpt-3.5-turbo",\n    messages=[\n      {"role": "system", "content": "Hello, you are a helpful scientific assistant."},\n      {"role": "user", "content": "What is the chemical composition of sugar?"}\n    ],\n    stream=True,\n)\n\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or "", end="")\n\n'))),(0,r.yg)(o.A,{value:"py3",label:"Azure OpenAI",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell\x3c!--"},"pip install openai\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-py"},'from openai import AzureOpenAI\nimport os\n\n# Javelin Headers\njavelin_api_key = os.environ[\'JAVELIN_API_KEY\']\nllm_api_key = os.environ["AZURE_OPENAI_API_KEY"]\n\njavelin_headers = {\n                    "x-api-key": javelin_api_key,     # Javelin API key from admin\n                    "x-javelin-route": "sampleroute1" # Javelin route to use\n                  }\n\nclient = AzureOpenAI(api_key=llm_api_key,\n                     base_url="https://api.javelin.live/v1/query",\n                     default_headers=javelin_headers,\n                     api_version="2023-07-01-preview")\n\ncompletion = client.chat.completions.create(\n  model="gpt-3.5-turbo",\n  messages=[\n    {"role": "system", "content": "Hello, you are a helpful scientific assistant."},\n    {"role": "user", "content": "What is the chemical composition of sugar?"}\n  ]\n)\n\nprint(completion.model_dump_json(indent=2))\n\n# Streaming Responses\nstream = client.chat.completions.create(\n    model="gpt-3.5-turbo",\n    messages=[\n      {"role": "system", "content": "Hello, you are a helpful scientific assistant."},\n      {"role": "user", "content": "What is the chemical composition of sugar?"}\n    ],\n    stream=True,\n)\n\nfor chunk in stream:\n  if chunk.choices:\n    print(chunk.choices[0].delta.content or "", end="")\n\n'))),(0,r.yg)(o.A,{value:"py4",label:"LangChain",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell\x3c!--"},"pip install langchain\npip install langchain-openai\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-py"},'from langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nimport os\n\njavelin_api_key = os.getenv(\'JAVELIN_API_KEY\')\nllm_api_key = os.getenv("OPENAI_API_KEY")\njavelin_headers = {\n                    "x-api-key": javelin_api_key,      # Javelin API key from admin\n                    "x-javelin-route": "sample_route1" # Javelin route to use\n                  }\n\nllm = ChatOpenAI(\n    openai_api_base="https://api.javelin.live/v1/query",\n    openai_api_key=llm_api_key,\n    model_kwargs={\n      "extra_headers": javelin_headers\n    },\n)\n\nprompt = ChatPromptTemplate.from_messages([\n    ("system", "Hello, you are a helpful scientific assistant."),\n    ("user", "{input}")\n])\n\noutput_parser = StrOutputParser()\n\nchain = prompt | llm | output_parser\n\nprint(chain.invoke({"input": "What is the chemical composition of sugar?"}))\n'))),(0,r.yg)(o.A,{value:"py5",label:"DSPy",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell\x3c!--"},"pip install dspy-ai\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-py"},'import dspy\nfrom dsp import LM\nimport os\nimport requests\n\n# Assuming the environment variables are set correctly\njavelin_api_key = os.getenv(\'JAVELIN_API_KEY\')\nllm_api_key = os.getenv("OPENAI_API_KEY")\n\nclass Javelin(LM):\n    def __init__(self, model, api_key):\n        self.model = model\n        self.api_key = api_key\n        self.provider = "default"\n        self.kwargs = { \n                    "temperature": 1.0, \n                    "max_tokens": 500, \n                    "top_p": 1.0, \n                    "frequency_penalty": 0.0, \n                    "presence_penalty": 0.0, \n                    "stop": None, \n                    "n": 1, \n                    "logprobs": None, \n                    "logit_bias": None,\n                    "stream": False\n        }\n\n        self.base_url = "https://api.javelin.live/v1/query/"\n        self.javelin_headers = {\n                    "Content-Type": "application/json",\n                    "Authorization": f"Bearer { api_key }",\n                    "x-javelin-route": "openai", # route name configured for OpenAI\n                    "x-api-key": javelin_api_key,\n        }\n\n        self.history = []\n\n    def basic_request(self, prompt: str, **kwargs):\n        headers = self.javelin_headers\n\n        data = {\n            **kwargs,\n            "model": self.model,\n            "messages": [\n                {"role": "user", "content": prompt}\n            ]\n        }\n\n        response = requests.post(self.base_url, headers=headers, json=data)\n        response = response.json()\n\n        self.history.append({\n            "prompt": prompt,\n            "response": response,\n            "kwargs": kwargs,\n        })\n        return response\n\n    def __call__(self, prompt, only_completed=True, return_sorted=False, **kwargs):\n        response = self.request(prompt, **kwargs)\n        if \'choices\' in response and len(response[\'choices\']) > 0:\n            first_choice_content = response[\'choices\'][0][\'message\'][\'content\']\n            completions = [first_choice_content]\n            return completions\n        else:\n            return ["No response found."]\n\njavelin = Javelin(model="gpt-4-1106-preview", api_key=llm_api_key)\ndspy.configure(lm=javelin)\n\n# Define a module (ChainOfThought) and assign it a signature (return an answer, given a question).\nqa = dspy.ChainOfThought(\'question -> answer\')\nresponse = qa(question="You have 3 baskets. The first basket has twice as many apples as the second basket. The third basket has 3 fewer apples than the first basket. If you have a total of 27 apples, how many apples are in each basket?")\nprint(response)\n'))),(0,r.yg)(o.A,{value:"py6",label:"More Libraries & Frameworks...",mdxType:"TabItem"},(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"https://docs.datastax.com/en/ragstack/docs/index.html"},"DataStax RAGStack"))),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"https://jxnl.github.io/instructor/"},"Instructor, Generating Structure from LLMs"))))),(0,r.yg)("h3",{id:"javascripttypescript"},"JavaScript/TypeScript"),(0,r.yg)(l.A,{mdxType:"Tabs"},(0,r.yg)(o.A,{value:"js1",label:"OpenAI",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell"},"npm install openai\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-js"},'import OpenAI from "openai";\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n  baseURL: "https://api.javelin.live/v1/query",\n  defaultHeaders: {\n    "x-api-key": `${process.env.JAVELIN_API_KEY}`,\n    "x-javelin-route": "sample_route1",\n  },\n});\n\nasync function main() {\n  const completion = await openai.chat.completions.create({\n    messages: [{ role: "system", content: "You are a helpful assistant." }],\n    model: "gpt-3.5-turbo",\n  });\n\n  console.log(completion.choices[0]);\n}\n\nmain();\n\n'))),(0,r.yg)(o.A,{value:"js2",label:"Langchain",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell"},"npm install @langchain/openai\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-js"},'\nimport { ChatOpenAI } from \'@langchain/openai\';\n\nconst llm = new ChatOpenAI({\n    openAIApiKey: process.env.OPENAI_API_KEY,\n    configuration: {\n        basePath: "https://api.javelin.live/v1/query",\n        defaultHeaders: {\n          "x-api-key": `${process.env.JAVELIN_API_KEY}`,\n          "x-javelin-route": "sample_route1",\n        },\n    },\n});\n\nasync function main() {\n  const response = await llm.invoke("tell me a joke?");\n  console.log(response);\n}\n\nmain();\n\n'))),(0,r.yg)(o.A,{value:"js3",label:"More Libraries & Frameworks...",mdxType:"TabItem"},(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("a",{parentName:"p",href:"https://sdk.vercel.ai/docs"},"Vercel AI SDK")),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"https://vercel.com/blog/ai-integrations"},"AI Integrations on Vercel")))),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("a",{parentName:"p",href:"https://github.com/huggingface/chat-ui"},"Hugging Face ChatUI")))))))}g.isMDXComponent=!0}}]);